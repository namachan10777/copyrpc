%%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,eabstractは任意)
%% [noauthor]
%%

%\documentclass[submit,techrep]{ipsj}
\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage[dvips]{graphicx}
\usepackage{latexsym}

\begin{document}

\title{LocustaRPC: 次世代リーダーシップマシンのための\\スケーラブルなRPC基盤}

\etitle{LocustaRPC: A Scalable RPC Framework\\for Next-Generation Leadership Machines}

\affiliate{UT}{筑波大学\\
University of Tsukuba}

\author{中野 将生}{Masaki Nakano}{UT}
\author{前田 宗則}{Munenori Maeda}{UT}
\author{建部 修見}{Osamu Tatebe}{UT}

\begin{abstract}
次世代リーダーシップマシンでは数十万ノード規模のシステムが想定されており，
従来のRPC基盤ではスケーラビリティが課題となる．
本稿では，大規模システム向けのスケーラブルなRPC基盤であるLocustaRPCを提案する．
LocustaRPCは，RDMAを活用した低遅延通信と，
スケーラブルなコネクション管理機構を備える．
評価の結果，既存手法と比較して高いスケーラビリティを達成することを確認した．
\end{abstract}

\maketitle

\section{はじめに}

\section{関連研究}

\section{設計}

現代のHPCノードは、富岳NEXT、Sirius、El capitanといったようにノード内並列性を向上させつつノード数自体は抑えるFat node設計が増えてきている。
ムーアの法則が半導体プロセスの改善とマイクロアーキテクチャ改良によるクロック、IPCの向上よりも、ノード内並列性向上の寄与が大きくなり変質してきたことがその理由である。
そのため、現代的な設計ではノード数自体はむしろ減少し、ネットワークトポロジとしては高い性能が標榜されている一方、
通信ライブラリにおいては未だ個別のクライアントがNICを直接利用する設計が多く、ここにギャップが存在する。
% RDMAXとかのHWによるスケーラビリティと比較して検討する

そこで、本研究においては、計算ノード上でのミドルウェア展開という枠組みを踏まえ、以下を目的としたミドルウェア設計の提案を行う。

\begin{enumerate}
    \item Daemonへのリクエスト集約
    \item Daemonへのリクエスト移譲による接続数節約と、それによるメモリ使用量・スループットの改善
    \item 通信のメモリアクセスパイプライン最適化と、sfence集約によるIPC向上
\end{enumerate}

\subsection{RPC設計}

通常POSIXインターフェースでは同期的アクセスとなるので、1 node中のリクエストを集約してもQueue depthはあまり稼げない。
% できれば: 既存のHPCアプリでの命令をカウントしておく
重要であるのは、RQ/SQをリソース上のボトルネックから外してRNR retryを防ぐこと、レイテンシを最小化しつつスループットを向上させることである。
% 関連研究を引く。多分one-sided擁護が多いので困らないはず。ただし、これらはibverbsのオーバーヘッドが原因というのは注意
設計上でノード内リクエストの集約によって接続数を絞るので、N-to-N通信ではなく、1-to-1通信をノードごとに用意し、end-to-endのflow controlによって確実な通信を行う。
3000ノード級に対応するため、CQEにイベントを集約してpollingコストを減らす。
従来の研究ではバッファ上にvalidフラグをおいてそこをpollingする手法が提案されていたが、これはibverbsのAPIがthread safetyを提供するために各種オーバーヘッドが大きかったためであり、
CQEもバッファも同じくNICが書き換えるメモリのPollingに過ぎず、現代のアーキテクチャにおいてはMESIFの最適化により書き換えは重大なボトルネックとはならない。
% 従来研究のサーベイ論文があったのでそれを引く。で、CQEに対するDDIOを分析。あとFast-ForwardでのI/E遷移の話もしよう。
% I/E遷移より実はL3→L1昇格の方が若干だけ速いってのも重要な話
% 出来れば比較をする
むしろ、ポーリング領域を一本に集約し、NIC HWに通知集約をオフロードすることによる性能向上が重要である。

そこで、バッファ管理コストを低減し高速な通信を行うため、WRITE with Immediateを中心に設計する。

\subsubsection{基本設計}

call/responseのみをモデルとしてサポートし、送信リング、受信リングでのバッファ間転送としてRPCを実現する。
逆方向の通信でも同様にリングを使うので、それぞれエンドポイントは対向あたり送受信2つの固定長リングバッファを持つ。
受信はWrite with ImmediateのImmediate通知に集約し、Immediate 32 bitをEndpoint IDとする。
また、それぞれのbulk messageの先頭には単調増加するPiggybacked seq, message countをもち、個別のメッセージについては、
len, payloadを持つ。受信した際はImmediateでEndpoint IDを直接特定し、
先頭を読んでPiggybacked seqでフロー管理を更新し、それ以降の受信メッセージをmessage countに達するまで読み込む。

seqは受信して上書き可能になったメッセージの容量であり、バッファはメッセージごとに64B alignされているので64 B blockの数として表現される。
このSeqはそれぞれのバッファリングされた返信の書き込みの際に、最新の値を先頭に入れることでFlow controlを行う。
ただし、これではデッドロックに陥るケースが存在する。
Seqの通知を受けない限りバッファ解放を知れないため通信が出来ないが、Seqを送るには対向側に空きが存在する必要がある。
そこで、リクエストごとにレスポンス許容量を設定する。このレスポンス許容量と、現在の受信バッファの空きを見て、リクエストを送れるかを判断する。

\subsubsection{Req/Res同居型}

まずReqとして送れるInflight Buffer blockの数を制限する。これは設定可能だが、デフォルトではリングの半分とする。
まずReqを送る際に、このReq用のInflight容量を超えていないかをチェックする。超えていればRing fullでエラー。
これに加えて、InflightなReqに設定されたResponse許容量の合計から、Responseの容量予約が取れるかを確認する。
取れなければここでもエラー。これにより、厳密なFlow controlの進行性を保証する。

\section{実装}

\section{評価}

\section{おわりに}

\begin{acknowledgment}
謝辞をここに記述する．
\end{acknowledgment}

\bibliographystyle{ipsjsort}
\bibliography{filesystem}

\end{document}

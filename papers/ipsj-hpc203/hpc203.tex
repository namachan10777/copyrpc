%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,eabstractは任意)
%% [noauthor]
%%

%\documentclass[submit,techrep]{ipsj}
\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage[dvipdfmx]{graphicx}
\usepackage{latexsym}
\usepackage{url}

% ipsjクラスの\doiがDOI中のアンダースコアを処理できない問題を回避
\makeatletter
\renewcommand{\doi}[1]{\url{https://doi.org/#1}}
\makeatother

\begin{document}

\title{LocustaRPC: 次世代リーダーシップマシンのための\\スケーラブルなRPC基盤}

\etitle{LocustaRPC: A Scalable RPC Framework\\for Next-Generation Leadership Machines}

\affiliate{UT}{筑波大学}
\affiliate{CCS}{筑波大学計算科学研究センター}
\affiliate{FUJITSU}{富士通株式会社}

\author{中野 将生}{Masaki Nakano}{UT}
\author{前田 宗則}{Munenori Maeda}{CCS,FUJITSU}[maeda.munenori@fujitsu.com]
\author{建部 修見}{Osamu Tatebe}{CCS}

\begin{abstract}
HPCマシンにおける性能向上は，アクセラレータ
\end{abstract}

\maketitle

\section{はじめに}

ムーアの法則はパイプラインの深化やOut-of-Order実行に代表されるマイクロアーキテクチャの改良，半導体製造プロセスの改良の両輪によって進んできていた．
しかし，物理的限界によってシングルコアでの性能向上が困難になったことで，マルチコア化やアクセラレータによる一部処理の特殊ハードウェア化が進み，
HPCにおいてもアクセラレータ，メニーコアといった構成が採用されてきた．近年では，El capitan，Frontierといったリーダーシップマシンでは % El capitan, Frontierについてそれぞれ設計を発表するProceedingsなどを引用出来るとベスト
アクセラレータとCPUソケットを複数搭載したFat-nodeと呼ばれる構成が採用されている．
日本においても，筑波大学のSiriusスーパーコンピュータ，富岳NEXTにおいてFat-node構成が採用されており， % Siriusについては多分まだ発表はない．富岳NEXTはスライドを引用する
ノード数自体は減少しつつも全体として高い並列性を持ち高度な計算能力が実現される．

一方で，こうした構成ではインターコネクトの物理設計としてはスケーラビリティを確保するものの，
ソフトウェア的にはプロセス数自体はむしろ増加するため通信ライブラリでのスケーラビリティについての課題が残る．
富岳NEXTでは100万オーダーでのプロセス数が想定されており，% 松岡先生のスライドから引用する
ad-hocファイルシステムではPOSIXインターフェースを介した柔軟なI/O要求を処理する必要があるため，どのクライアントがどのサーバにアクセスするかを事前に強く制約しにくい．
このため全体での接続性を維持する設計を採ると，1ノードあたり100万オーダーの接続数受け入れが必要になってくる．
数値計算においては，NCCLやMPIといった通信ミドルウェアでの最適化研究の結果この問題は緩和されているが， % 論文を引用する
ストレージ等のHPCアプリを補助するミドルウェアでの通信では，アクセス先の偏りや通信相手を固定的に仮定しにくく，これらの最適化を直接適用することは難しい．
MPI-IOのようなアクセスパターンの仮定が可能なミドルウェアでは高いスケーラビリティを達成する手法が提案されている一方， % Kohei Hiraga, Kohei Sugiharaらの研究を引用
HPCアプリではPOSIXインターフェースを備える並列ファイルシステムを仮定したアプリも多い．

天文データ解析，機械学習，大気シミュレーションといった高いストレージ性能を要求するアプリへの回答として，
富岳ではLLIOが導入され並列ファイルシステムの性能を補完している．% LLIOを引用
このような計算ノードでの一時的なストレージミドルウェアはad-hocファイルシステムとして研究が行われている．% Gecko, CHFS, BeeOND, DeltaFSあたりは最低でも引用する．
ad-hocファイルシステム研究では主に並列ファイルシステムでボトルネックとなりがちであった
メタデータのスケーラビリティ改善が達成されてきた．
ただ，これらの研究は主に分散アルゴリズムが主眼であり，通信層の最適化は副次的なものとなっている．

こうした通信層についてのスケーラビリティ上の課題は，データセンター内データベースの最適化問題として先行して研究が行われてきた．
一方でHPC向けストレージミドルウェアでは，扱うデータ量や同時通信の規模が大きく，KVSに比べてクエリの単純化も難しいため，通信アーキテクチャ自体の改良がより重要になる．
HPCで広く用いられるInfinibandネットワークでは，
高速かつ高信頼性のRC接続による通信によって分散計算やストレージミドルウェアが実装されるが，
このRC接続の状態管理がネットワークインターフェースにおいてスケーラビリティの課題を抱えていることが報告されている． % ScaleRPCを引用する．あとKalia先生も多分報告していたはず．ScaleRPC，RDMAX，UD系RPCはこうした文脈で研究されているので．
また，単純なネットワーク性能のみならず通信のためのメモリリソースも計算用メモリの圧迫としてアプリ性能に影響を与えうる． % ここ引用がほしい
従来のHPC向けRPCフレームワークでは，クライアントが直接接続を持つためノード内プロセスの増加がそのまま
リソースの増加に直結する構造であり，Fat-nodeになりノード数が減少しても問題は解決されない．

本研究では，大規模なHPCアプリ実行時に，ad-hocファイルシステムのようなミドルウェアで接続状態の増大により通信性能低下とメモリ消費増大が同時に生じる点を課題とする．
とりわけメモリ消費増大は，HPCノードでOOM killerによる強制終了を誘発し，計算用メモリを直接圧迫するため深刻である．
この課題に対し，POSIX共有メモリを用いたノード内通信によってコネクションを集約し，従来のHPC向けRPCより少ないリソースでノード間通信を実現するアーキテクチャを提案する．
さらに，提案アーキテクチャによって検証用Key-Valueストアを実装して，設計の有効性を検証する．
本稿の貢献は以下の通りである．

\begin{enumerate}
	\item 共有メモリを用いた通信集約アーキテクチャ
	\item 共有メモリ上に集約されたリクエスト転送を目的としたRPCの設計とフローコントロール
	\item リクエスト代行におけるバッチング崩壊問題と，その対処のための制御アルゴリズム
	\item ベンチマーク用Key-Value Storeによる性能比較
\end{enumerate}

\section{背景}

\subsection{並列ファイルシステムの発展とad-hocファイルシステム}

並列ファイルシステムは，メタデータ管理とデータ管理を分離し，データを複数ストレージノードに分散配置して並列帯域を引き出す構成を基本として発展してきた．
PVFSやLustreに代表される設計は，大規模チェックポイントのような大容量I/Oで高い実効帯域を達成し，HPCの標準的な永続ストレージ層として定着している\cite{carnsPVFSParallelFile2000}．
しかし，計算性能の向上に対してストレージ性能の伸びは相対的に遅く，計算とI/Oの性能ギャップが継続的に拡大していることが指摘されている\cite{tatebeGfarmBBGfarm2020}．
このギャップは，機械学習やデータ解析ワークロードに見られる小粒度I/Oやメタデータ集中アクセスで特に顕在化し，共有ストレージ単体では安定した低遅延処理が難しくなる\cite{brinkmannAdHocFile2020}．

こうした背景から，計算ノード上のNVMe SSDや永続メモリをジョブ実行中に束ねる中間層として，バーストバッファおよびad-hocファイルシステムが発展してきた\cite{brinkmannAdHocFile2020}．
\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/pfs-and-adhocfs.drawio.pdf}
	\caption{並列ファイルシステムとad-hocファイルシステムの構成比較}
	\label{fig:pfs-adhocfs-overview}
\end{figure}
建部らのGfarm/BBは，ノードローカルストレージを用いる一時的オンデマンドファイルシステムとして，ジョブ開始前に高速構築し，ジョブ終了時に破棄する運用を前提としている\cite{tatebeGfarmBBGfarm2020}．
同研究は，この層で重視すべき要件を「永続性や冗長性よりもアクセス性能とメタデータ性能」と定義し，RDMAやファイル記述子受け渡しを活用した高速化で有効性を示している\cite{tatebeGfarmBBGfarm2020}．

また，CHFSはad-hocファイルシステムを永続メモリ向けに再設計し，専用メタデータサーバを置かず，一貫性ハッシュを用いた分散Key-Value Store上にファイルシステム機能を直接構築することで，
ノード数増加時のメタデータ性能とデータアクセス性能の両方を改善している\cite{tatebeCHFSParallelConsistent2022}．
CHFSが示す設計上の要点は，(1)集中管理点の排除，(2)逐次実行パスの削減，(3)ノードローカル媒体の特性に合わせた軽量データ構造であり，
ad-hocファイルシステムのスケーラビリティを議論する上で重要な指針となる\cite{tatebeCHFSParallelConsistent2022}．
さらに，GekkoFS，BurstFS，UnifyFS，DeltaFSに代表される研究も，ワークロード特性に応じてメタデータ管理方式や同期モデルを見直すことで，従来PFSのボトルネックを補完する方向を示している\cite{vefGekkoFSTemporaryDistributed2018,wangBurstFSDistributedBurst2016,brimUnifyFSUserlevelShared2023,zhengDeltaFSExascaleFile2015,zhengDeltaFSScalableNogroundtruth2021}．
杉原らのMPI-IO研究が示すように，既存アプリケーション互換性を維持したままI/O経路の局所性を高める工夫も，引き続き実用上重要である\cite{sugiharaDesignLocalityawareMPIIO2020}．

以上を踏まえると，ad-hocファイルシステムに求められる要件は次の3点に整理できる．
\begin{enumerate}
	\item 既存HPCアプリケーションを活かすためのPOSIX互換インターフェース，またはそれに準ずる透過性
	\item ノード内外で大量に発生する小粒度リクエストを処理するための高いメタデータ性能と低遅延通信
	\item 計算用資源を圧迫しない実装，特に計算ノードメモリ消費を抑えた通信基盤
\end{enumerate}
本研究は，このうち3点目の通信資源効率に焦点を当て，ad-hocファイルシステムの実用スケールでボトルネックとなる接続状態とメモリ消費の問題を扱う．

\subsection{RDMA通信}

RDMAは，CPU介在を抑えつつ低遅延・高帯域のデータ転送を実現する通信機構である．
実装上は，送信キュー（SQ）と受信キュー（RQ）を対にしたQueue Pair（QP）に対してWork Request（WR）を投稿し，
NICがこれをWork Queue Element（WQE）として処理する．
完了通知はCompletion Queue（CQ）に集約され，アプリケーションはCQをポーリングして通信進行を把握する．
この関係を図\ref{fig:rdma-sq-rq}に示す．

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/RDMA-SQ-and-RQ.drawio.pdf}
	\caption{RDMAにおけるSQ，RQ，QPの関係}
	\label{fig:rdma-sq-rq}
\end{figure}

通信開始時，ホストはSQ上にWQEを配置した後，doorbellレジスタへのMMIO書き込みでNICへ新規WQEの存在を通知する．
このdoorbell発行は固定コストを持つため，複数リクエストをまとめて投稿するdoorbell batchingや，ペイロードのinline化によってPCIeトランザクションを削減する設計が有効である\cite{kalia2016DesignGuidelinesHigh}．
また，WQEを毎回完了通知付きで発行するとCQ処理負荷が増えるため，実運用ではsignaled/unsignaledを組み合わせて制御するのが一般的である\cite{kalia2016DesignGuidelinesHigh}．

RDMAのトランスポートにはRC（Reliable Connection）やUD（Unreliable Datagram）などがある．
RCは信頼性と順序性を提供し，大きなデータ転送や厳密なフロー制御が必要なミドルウェア実装と親和性が高い一方，接続状態の管理コストが課題となる\cite{chenScalableRDMARPC2019,kong2023UnderstandingRDMAMicroarchitecture}．
本研究が対象とするad-hocファイルシステムのような環境では，大量の小粒度制御メッセージとデータ転送が混在するため，
RCの利点を活かしつつ接続数とメモリ消費を抑えるアーキテクチャが必要となる．

RC，UD，DCの特徴を，スケーラビリティの観点で整理すると表\ref{tab:rdma-transports}のようになる．
UD（Unreliable Datagram）は接続状態を持たないため接続数起因の状態量増大を避けやすいが，信頼性や順序制御をソフトウェア側で補う必要がある．
DC（Dynamically Connected）は，通信時に接続先を動的に切り替えて見かけ上の接続数を削減できるが，
内部的にはRC系の接続状態管理に依存しており，状態管理コストを根本的に消すものではない．
そのため，DCはRCのスケーラビリティ問題を緩和する有力な手段ではあるが，完全な解決策としては扱えない点に注意が必要である．

\begin{table}[t]
	\caption{RDMAトランスポートの比較（スケーラビリティ観点）}
	\label{tab:rdma-transports}
	\centering
	\begin{tabular}{l|p{0.22\linewidth}|p{0.24\linewidth}|p{0.26\linewidth}}
		\hline
		方式 & 主な利点 & 主な制約 & スケーラビリティ上の論点 \\
		\hline
		RC & 信頼性・順序性・one-sided通信が利用可能 & 接続ごとの状態をNIC/メモリに保持 & 接続数増加で状態量とキャッシュ競合が増えやすい  \\ \hline
		UD & 接続レスで状態量を抑えやすい & 信頼性・順序制御・再送をソフトウェアで実装 & 大規模化しやすい一方，ミドルウェア設計負担が増加  \\ \hline
		DC & 見かけ上の接続数を圧縮しやすい & 実装複雑性と切替コスト，機能依存性 & RC状態管理の緩和は可能だが根本解消ではない  \\
		\hline
	\end{tabular}
\end{table}

\section{関連研究}

\subsection{FaRM，eRPC，ScaleRPC}

ソフトウェアによるRDMA RPCのスケーラビリティ改善は，主に「接続状態の扱い」「受信処理の検出コスト」「メモリ階層競合の抑制」という3つの観点で進展してきた．
FaRMはone-sided RDMA read/writeを中核に据え，RDMA writeベースのメッセージングをリングバッファとポーリングで実装した\cite{dragojevicFaRMFastRemote2014}．
同研究は高い性能を示す一方で，チャネル数増加に伴うポーリング負荷と，接続増加に伴うNIC上のQueue Pair状態キャッシュ不足を報告しており，
対策として(1)接続粒度をthread-to-threadからthread-to-nodeへ粗粒化し，(2)NUMAを意識したQP共有を導入して状態量を削減している\cite{dragojevicFaRMFastRemote2014}．
すなわちFaRMのアプローチは，RCベースの高速性を維持しつつ，接続の束ね方をソフトウェアで最適化する方向である．
これに対し本研究は，スレッド単位ではなくノード内の複数プロセス単位で通信を集約する点で異なる．

eRPCは，接続状態をNICに保持するRC系設計そのものをスケーラビリティ制約と捉え，packet I/O（UDP/UD相当）を用いてNIC管理状態を削減する設計を採る\cite{kalia2019DatacenterRPCsCan}．
論文中では，RDMA接続数が増えるとNIC SRAM上の接続状態キャッシュ制約により性能が低下することを示し，eRPCでは接続状態をCPU側で管理することで大規模接続時の性能維持を図っている\cite{kalia2019DatacenterRPCsCan}．
加えて，event loopによるdispatch処理，session creditによるエンドツーエンドフロー制御，zero-copy送受信を組み合わせることで，
汎用RPCとしての機能性と高スループットを両立している\cite{kalia2019DatacenterRPCsCan}．
UD系アプローチで性能が伸びる主要因は，(1)NIC側の接続状態保持を削減できること，(2)受信検出をCQベースで定数時間化しやすいこと，(3)接続切替に伴うNICキャッシュミスを避けやすいことである\cite{kalia2019DatacenterRPCsCan}．
一方で，信頼性制御や再送制御，および大きなデータ転送時のバッファ管理をソフトウェア側で担う必要があり，ワークロードによっては設計複雑性が増す．

ScaleRPCは，RCを維持したままスケーラビリティを高めるため，接続を時間分割で扱うconnection groupingと，
物理メッセージプールを論理的に多重化するvirtualized mappingを提案している\cite{chenScalableRDMARPC2019}．
前者は同時にアクティブ化する接続群を制限してNICキャッシュのスラッシングを抑え，後者はメッセージバッファ共有によってCPU最終段キャッシュ競合とメモリ使用量を削減する\cite{chenScalableRDMARPC2019}．
さらに，優先度ベースのスケジューリングとwarmup機構によりグループ切替オーバーヘッドを隠蔽し，RCの利点を保ちながら大規模接続への適応を図っている\cite{chenScalableRDMARPC2019}．

これら3研究は，いずれもソフトウェアでの工夫によりRDMA RPCのスケーラビリティを改善しているが，設計思想は異なる．
FaRMとScaleRPCはRCを前提に接続状態管理の効率化を狙うのに対し，eRPCはNIC接続状態依存を減らす方向で拡張性を確保する．
また，これらの多くはデータセンター内KVS/DB系ワークロードを主対象としており，要求メッセージが小さく制御可能である前提が強い．
HPC向けad-hocファイルシステムでは，計算ノード内でクライアントとストレージデーモンが同居し，大量同時アクセスと大きなデータ転送が混在するため，
サーバ側だけでなくクライアント側を含む通信資源管理が重要になる．
FaRMの最適化単位はスレッド中心であり，eRPCは汎用性を重視する代わりにone-sided操作前提のデータ転送管理を直接提供しないため，
HPCストレージ向け実装では追加の設計判断が必要となる．
本研究はad-hocファイルシステムを想定し，RCの機能を利用しつつノード内集約で接続状態とメモリ消費を削減する点で，RC維持型アプローチをHPCストレージ文脈に適用する位置づけとなる．

\subsection{mRPC}

mRPCは，従来の「アプリ内RPCライブラリ＋サイドカー」構成における重複した(アン)マーシャリングを問題設定とし，
RPCを管理対象のシステムサービスとして提供するアーキテクチャを提案している\cite{chen2023RemoteProcedureCall}．
同研究の中核は，マーシャリングとポリシー適用を同一の特権サービス側に集約し，アプリケーションは共有メモリキュー経由でRPC要求を受け渡す点にある\cite{chen2023RemoteProcedureCall}．
これにより，サイドカー方式で生じるデータコピーと再マーシャリングを削減しつつ，アクセス制御・レート制御・可観測性機能を実行時に挿入できる．

設計面では，mRPCサービス内部をポリシー，トランスポート，フロントエンド等のエンジンに分解し，
エンジンの動的バインディングとライブアップグレードを可能にしている\cite{chen2023RemoteProcedureCall}．
この機構により，アプリケーションを再コンパイル・再起動せずに，ポリシー，トランスポート，マーシャリング実装を更新可能とする点が特徴である．
評価では，gRPC+EnvoyやeRPC+proxyに対して，特にサイドカー経由時の冗長処理を削減することで，レイテンシとスループットの改善を報告している\cite{chen2023RemoteProcedureCall}．

本研究との類似点は，ノード内でRPC処理をサービス側に寄せ，アプリケーション側との通信に共有メモリを活用することで，
ネットワーク経路へ投入する前の処理を集約する点にある．
一方で目的は大きく異なる．mRPCの主眼はクラウド/マイクロサービス環境における運用性（ポリシー柔軟性，可観測性，無停止更新）の向上であり，
接続状態量やRC通信資源の削減そのものは第一目的ではない\cite{chen2023RemoteProcedureCall}．
これに対し本研究は，HPC向けad-hocファイルシステムにおいて，ノード内の複数プロセスから生じる通信を集約し，
接続状態とメモリ消費を抑えて大規模実行時の通信スケーラビリティを確保することを主目的とする．

\section{設計}

\subsection{アーキテクチャ}

提案アーキテクチャでは，ノード外通信の終端を通信デーモンに集約し，アプリケーションプロセスは共有メモリ経由でデーモンへ要求を委譲する．
ここで通信デーモンとサービス処理スレッドは同一プロセス内で動作する．

各アプリケーションプロセスはデーモンと共有する送信バッファを持つ．
この送信バッファは要求専用ではなく，クライアントによるリクエスト書き込みとデーモンによるレスポンス書き込みが同居する双方向領域である．
したがって，この領域は単純な一方向キューではなく，呼び出し単位で確保されたスロットを介して要求と応答を対応付ける．

リモート呼び出し時，クライアントは送信バッファ内の割当済み領域へ要求を直接書き込む．
デーモンは要求内容そのものを解釈してディスパッチするのではなく，どこまで要求が書き込まれたかという進行位置を管理し，
未送信領域に対応するWQEを構築してdoorbellを発行する．
すなわち，デーモンの役割は主に転送境界管理とRDMA送信発行である．
受信した応答はデーモンが対応スロットへ書き戻し，クライアントは自スロットの状態遷移を観測して完了を判定する．

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/in-node-architecture.drawio.pdf}
	\caption{ノード内アーキテクチャとReq/Response書き込みの流れ}
	\label{fig:in-node-architecture}
\end{figure}

一方，ローカルノードで完結する要求はこのリモート呼び出し経路を通さない．
ローカル専用のIPCキューを別途用意し，ノード内処理はそのIPCキューで処理することで，リモート経路の送信バッファを汚染しない設計とする．

この構成により，ノード間接続，送受信バッファ，フロー制御状態をデーモン側へ集約できるため，
ノード内プロセス数が増えてもノード外通信状態量の増加を抑えられる．
結果として，Fat-node環境で問題となる接続状態メモリと通信制御オーバーヘッドの増大を緩和する．

\subsection{RDMA通信プロトコル}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\textwidth]{handmade-figures/copyrpc.pdf}
	\caption{copyrpcの基本通信フロー}
	\label{fig:copyrpc-overview}
\end{figure*}

\subsubsection{転送と通知}

本方式の基本方針は，クライアントが共有メモリ上の送信バッファへ要求を直接書き込み，
デーモンはその書き込み済み範囲をRDMA WRITEで転送し，WRITE with Immediate（WRITE+IMM）で受信側へ到着通知することである．
ここでデーモンは要求ペイロードを解釈しない．デーモンが扱うのは，送信バッファ上のどこまでが送信可能かという境界情報と，
対応するWQE列のみである．

クライアントがPOSIX共有メモリ上の送信バッファへ直接メッセージを用意することで，ノード内の余分なコピーと同期点を削減し，
デーモンはWQE生成とdoorbell発行に専念できる．

図\ref{fig:in-node-architecture}の送信バッファは，要求と応答が同居する双方向リング領域である．
各要求はスロット単位で管理され，デーモンは複数スロットをバッチ化して1回のRDMA WRITE+IMMで転送する．
受信側は集約された受信CQをポーリングし，1つのCQEをトリガとしてバッチ内の複数メッセージを処理する．
これにより，通知処理はメッセージ単位ではなくバッチ単位へ償却される．

送信バッファは，slot header（call\_id，generation，状態，要求長/応答長）とpayloadからなるスロット列として管理する．
この論理フォーマットと可視化手順は図\ref{fig:message-write-protocol}で合わせて示す．

\subsubsection{フロー制御}

本方式では，送信バッファの進行位置を管理することで受信バッファ利用量も同期的に制御されるため，
送信側の管理だけで受信側バッファが自然にあふれない構成になる．
すなわち，送信可能範囲は対向の受信済み位置により制約され，この範囲を超える書き込みは行わない．

ただし，この制御だけではデッドロックが起こりうる．
典型例は，双方が大きなResponseを書き戻そうとして，互いに相手側の空きを待つ状態で停止する場合である．
この停止を避けるため，本方式では対向のconsumed位置をRDMA READで直接取得する．
送信側は待機中にこのREADを発行して最新の空き情報を取得し，進行位置を前進させることで相互待ちを解消する．

\subsubsection{送信バッファ書き込み調停}

複数プロセスが同一送信バッファへ同時に書き込むため，書き込み領域の調停と公開順序の保証が必要になる．
本方式では「領域確保」と「公開」を分離した2段階プロトコルを採る．
全体の流れは図\ref{fig:message-write-protocol}に示す．

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/message-write.drawio.pdf}
	\caption{複数クライアントによるメッセージ協調書き込み}
	\label{fig:message-write-protocol}
\end{figure}

第1段階では，クライアントが原子的なfetch\_addでproducer位置を前進させ，書き込み区間を確保する．
この時点では他プロセスから当該スロットは不可視である．
第2段階で，クライアントはslot header（call\_id，generation，要求長/応答長）とpayloadを書き込み，
最後にstateをReqReadyへ更新して可視化する．
この更新順序により，DaemonはReqReadyを観測した時点で当該スロットの内容を一貫して参照できる．
Daemonは前回処理したconsumer位置から順にslot headerのstateを確認し，ReqReadyの連続区間を送信範囲としてWQEを生成する．

このとき重要なのは，デーモンがPayload内容を見て要求を抽出しない点である．
デーモンはSlot Headerの境界情報と状態遷移のみを用いて送信可能区間を決定する．
結果として，調停対象は「どのスロットが公開済みか」という管理情報に限定され，競合点を最小化できる．

ローカル完結要求は専用IPCキューへ投入されるため，この送信バッファ調停はリモート送信対象の要求にのみ適用される．
すなわち，ノード内delegationの競合制御とノード間RDMA送信の競合制御を分離し，それぞれ最小限の同期機構で実装する．

リング折返し時はgeneration counterを更新して同一オフセット再利用を識別し，
応答書き戻し時にもcall\_idと世代を照合することで，古い要求との取り違えを防ぐ．

\subsection{Batching制御}

\begin{acknowledgment}
本研究の一部は，JSPS科研費JP22H00509，
NEDO（国立研究開発法人新エネルギー・産業技術総合開発機構）の委託事業「ポスト5G情報通信システム基盤強化研究開発事業」（JPNP20017），
JST CREST JPMJCR24R4，
筑波大学計算科学研究センター学際共同利用プログラム，および富士通との特別共同研究の結果得られたものです．
\end{acknowledgment}

\bibliographystyle{ipsjsort}
\bibliography{filesystem}

\end{document}

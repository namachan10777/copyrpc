%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,eabstractは任意)
%% [noauthor]
%%

%\documentclass[submit,techrep]{ipsj}
\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage[dvipdfmx]{graphicx}
\usepackage{latexsym}
\usepackage{url}

% ipsjクラスの\doiがDOI中のアンダースコアを処理できない問題を回避
\makeatletter
\renewcommand{\doi}[1]{\url{https://doi.org/#1}}
\makeatother

\begin{document}

\title{LocustaRPC: 次世代リーダーシップマシンのための\\スケーラブルなRPC基盤}

\etitle{LocustaRPC: A Scalable RPC Framework\\for Next-Generation Leadership Machines}

\affiliate{UT}{筑波大学}
\affiliate{CCS}{筑波大学計算科学研究センター}
\affiliate{FUJITSU}{富士通株式会社}

\author{中野 将生}{Masaki Nakano}{UT}
\author{前田 宗則}{Munenori Maeda}{CCS,FUJITSU}[maeda.munenori@fujitsu.com]
\author{建部 修見}{Osamu Tatebe}{CCS}

\begin{abstract}
HPCマシンにおける性能向上は，アクセラレータ
\end{abstract}

\maketitle

\section{はじめに}

ムーアの法則はパイプラインの深化やOut-of-Order実行に代表されるマイクロアーキテクチャの改良，半導体製造プロセスの改良の両輪によって進んできていた．
しかし，物理的限界によってシングルコアでの性能向上が困難になったことで，マルチコア化やアクセラレータによる一部処理の特殊ハードウェア化が進み，
HPCにおいてもアクセラレータ，メニーコアといった構成が採用されてきた．近年では，El capitan，Frontierといったリーダーシップマシンでは % El capitan, Frontierについてそれぞれ設計を発表するProceedingsなどを引用出来るとベスト
アクセラレータとCPUソケットを複数搭載したFat-nodeと呼ばれる構成が採用されている．
日本においても，筑波大学のSiriusスーパーコンピュータ，富岳NEXTにおいてFat-node構成が採用されており， % Siriusについては多分まだ発表はない．富岳NEXTはスライドを引用する
ノード数自体は減少しつつも全体として高い並列性を持ち高度な計算能力が実現される．

一方で，こうした構成ではインターコネクトの物理設計としてはスケーラビリティを確保するものの，
ソフトウェア的にはプロセス数自体はむしろ増加するため通信ライブラリでのスケーラビリティについての課題が残る．
富岳NEXTでは100万オーダーでのプロセス数が想定されており，% 松岡先生のスライドから引用する
ad-hocファイルシステムではPOSIXインターフェースを介した柔軟なI/O要求を処理する必要があるため，どのクライアントがどのサーバにアクセスするかを事前に強く制約しにくい．
このため全体での接続性を維持する設計を採ると，1ノードあたり100万オーダーの接続数受け入れが必要になってくる．
数値計算においては，NCCLやMPIといった通信ミドルウェアでの最適化研究の結果この問題は緩和されているが， % 論文を引用する
ストレージ等のHPCアプリを補助するミドルウェアでの通信では，アクセス先の偏りや通信相手を固定的に仮定しにくく，これらの最適化を直接適用することは難しい．
MPI-IOのようなアクセスパターンの仮定が可能なミドルウェアでは高いスケーラビリティを達成する手法が提案されている一方， % Kohei Hiraga, Kohei Sugiharaらの研究を引用
HPCアプリではPOSIXインターフェースを備える並列ファイルシステムを仮定したアプリも多い．

天文データ解析，機械学習，大気シミュレーションといった高いストレージ性能を要求するアプリへの回答として，
富岳ではLLIOが導入され並列ファイルシステムの性能を補完している．% LLIOを引用
このような計算ノードでの一時的なストレージミドルウェアはad-hocファイルシステムとして研究が行われている．% Gecko, CHFS, BeeOND, DeltaFSあたりは最低でも引用する．
ad-hocファイルシステム研究では主に並列ファイルシステムでボトルネックとなりがちであった
メタデータのスケーラビリティ改善が達成されてきた．
ただ，これらの研究は主に分散アルゴリズムが主眼であり，通信層の最適化は副次的なものとなっている．

こうした通信層についてのスケーラビリティ上の課題は，データセンター内データベースの最適化問題として先行して研究が行われてきた．
一方でHPC向けストレージミドルウェアでは，扱うデータ量や同時通信の規模が大きく，KVSに比べてクエリの単純化も難しいため，通信アーキテクチャ自体の改良がより重要になる．
HPCで広く用いられるInfinibandネットワークでは，
高速かつ高信頼性のRC接続による通信によって分散計算やストレージミドルウェアが実装されるが，
このRC接続の状態管理がネットワークインターフェースにおいてスケーラビリティの課題を抱えていることが報告されている． % ScaleRPCを引用する．あとKalia先生も多分報告していたはず．ScaleRPC，RDMAX，UD系RPCはこうした文脈で研究されているので．
また，単純なネットワーク性能のみならず通信のためのメモリリソースも計算用メモリの圧迫としてアプリ性能に影響を与えうる． % ここ引用がほしい
従来のHPC向けRPCフレームワークでは，クライアントが直接接続を持つためノード内プロセスの増加がそのまま
リソースの増加に直結する構造であり，Fat-nodeになりノード数が減少しても問題は解決されない．

本研究では，大規模なHPCアプリ実行時に，ad-hocファイルシステムのようなミドルウェアで接続状態の増大により通信性能低下とメモリ消費増大が同時に生じる点を課題とする．
とりわけメモリ消費増大は，HPCノードでOOM killerによる強制終了を誘発し，計算用メモリを直接圧迫するため深刻である．
この課題に対し，POSIX共有メモリを用いたノード内通信によってコネクションを集約し，従来のHPC向けRPCより少ないリソースでノード間通信を実現するアーキテクチャを提案する．
さらに，提案アーキテクチャによって検証用Key-Valueストアを実装して，設計の有効性を検証する．
本稿の貢献は以下の通りである．

\begin{enumerate}
	\item 共有メモリを用いた通信集約アーキテクチャ
	\item 共有メモリ上に集約されたリクエスト転送を目的としたRPCの設計とフローコントロール
	\item リクエスト代行におけるバッチング崩壊問題と，その対処のための制御アルゴリズム
	\item ベンチマーク用Key-Value Storeによる性能比較
\end{enumerate}

\section{背景}

\subsection{並列ファイルシステムの発展とad-hocファイルシステム}

並列ファイルシステムは，メタデータ管理とデータ管理を分離し，データを複数ストレージノードに分散配置して並列帯域を引き出す構成を基本として発展してきた．
PVFSやLustreに代表される設計は，大規模チェックポイントのような大容量I/Oで高い実効帯域を達成し，HPCの標準的な永続ストレージ層として定着している\cite{carnsPVFSParallelFile2000}．
しかし，計算性能の向上に対してストレージ性能の伸びは相対的に遅く，計算とI/Oの性能ギャップが継続的に拡大していることが指摘されている\cite{tatebeGfarmBBGfarm2020}．
このギャップは，機械学習やデータ解析ワークロードに見られる小粒度I/Oやメタデータ集中アクセスで特に顕在化し，共有ストレージ単体では安定した低遅延処理が難しくなる\cite{brinkmannAdHocFile2020}．

こうした背景から，計算ノード上のNVMe SSDや永続メモリをジョブ実行中に束ねる中間層として，バーストバッファおよびad-hocファイルシステムが発展してきた\cite{brinkmannAdHocFile2020}．
\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/pfs-and-adhocfs.drawio.pdf}
	\caption{並列ファイルシステムとad-hocファイルシステムの構成比較}
	\label{fig:pfs-adhocfs-overview}
\end{figure}
建部らのGfarm/BBは，ノードローカルストレージを用いる一時的オンデマンドファイルシステムとして，ジョブ開始前に高速構築し，ジョブ終了時に破棄する運用を前提としている\cite{tatebeGfarmBBGfarm2020}．
同研究は，この層で重視すべき要件を「永続性や冗長性よりもアクセス性能とメタデータ性能」と定義し，RDMAやファイル記述子受け渡しを活用した高速化で有効性を示している\cite{tatebeGfarmBBGfarm2020}．

また，CHFSはad-hocファイルシステムを永続メモリ向けに再設計し，専用メタデータサーバを置かず，一貫性ハッシュを用いた分散Key-Value Store上にファイルシステム機能を直接構築することで，
ノード数増加時のメタデータ性能とデータアクセス性能の両方を改善している\cite{tatebeCHFSParallelConsistent2022}．
CHFSが示す設計上の要点は，(1)集中管理点の排除，(2)逐次実行パスの削減，(3)ノードローカル媒体の特性に合わせた軽量データ構造であり，
ad-hocファイルシステムのスケーラビリティを議論する上で重要な指針となる\cite{tatebeCHFSParallelConsistent2022}．
さらに，GekkoFS，BurstFS，UnifyFS，DeltaFSに代表される研究も，ワークロード特性に応じてメタデータ管理方式や同期モデルを見直すことで，従来PFSのボトルネックを補完する方向を示している\cite{vefGekkoFSTemporaryDistributed2018,wangBurstFSDistributedBurst2016,brimUnifyFSUserlevelShared2023,zhengDeltaFSExascaleFile2015,zhengDeltaFSScalableNogroundtruth2021}．
杉原らのMPI-IO研究が示すように，既存アプリケーション互換性を維持したままI/O経路の局所性を高める工夫も，引き続き実用上重要である\cite{sugiharaDesignLocalityawareMPIIO2020}．

以上を踏まえると，ad-hocファイルシステムに求められる要件は次の3点に整理できる．
\begin{enumerate}
	\item 既存HPCアプリケーションを活かすためのPOSIX互換インターフェース，またはそれに準ずる透過性
	\item ノード内外で大量に発生する小粒度リクエストを処理するための高いメタデータ性能と低遅延通信
	\item 計算用資源を圧迫しない実装，特に計算ノードメモリ消費を抑えた通信基盤
\end{enumerate}
本研究は，このうち3点目の通信資源効率に焦点を当て，ad-hocファイルシステムの実用スケールでボトルネックとなる接続状態とメモリ消費の問題を扱う．

\subsection{RDMA通信}

RDMAは，CPU介在を抑えつつ低遅延・高帯域のデータ転送を実現する通信機構である．
実装上は，送信キュー（SQ）と受信キュー（RQ）を対にしたQueue Pair（QP）に対してWork Request（WR）を投稿し，
NICがこれをWork Queue Element（WQE）として処理する．
完了通知はCompletion Queue（CQ）に集約され，アプリケーションはCQをポーリングして通信進行を把握する．
この関係を図\ref{fig:rdma-sq-rq}に示す．

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/RDMA-SQ-and-RQ.drawio.pdf}
	\caption{RDMAにおけるSQ，RQ，QPの関係}
	\label{fig:rdma-sq-rq}
\end{figure}

通信開始時，ホストはSQ上にWQEを配置した後，doorbellレジスタへのMMIO書き込みでNICへ新規WQEの存在を通知する．
このdoorbell発行は固定コストを持つため，複数リクエストをまとめて投稿するdoorbell batchingや，ペイロードのinline化によってPCIeトランザクションを削減する設計が有効である\cite{kalia2016DesignGuidelinesHigh}．
また，WQEを毎回完了通知付きで発行するとCQ処理負荷が増えるため，実運用ではsignaled/unsignaledを組み合わせて制御するのが一般的である\cite{kalia2016DesignGuidelinesHigh}．

RDMAのトランスポートにはRC（Reliable Connection）やUD（Unreliable Datagram）などがある．
RCは信頼性と順序性を提供し，大きなデータ転送や厳密なフロー制御が必要なミドルウェア実装と親和性が高い一方，接続状態の管理コストが課題となる\cite{chenScalableRDMARPC2019,kong2023UnderstandingRDMAMicroarchitecture}．
本研究が対象とするad-hocファイルシステムのような環境では，大量の小粒度制御メッセージとデータ転送が混在するため，
RCの利点を活かしつつ接続数とメモリ消費を抑えるアーキテクチャが必要となる．

RC，UD，DCの特徴を，スケーラビリティの観点で整理すると表\ref{tab:rdma-transports}のようになる．
UD（Unreliable Datagram）は接続状態を持たないため接続数起因の状態量増大を避けやすいが，信頼性や順序制御をソフトウェア側で補う必要がある．
DC（Dynamically Connected）は，通信時に接続先を動的に切り替えて見かけ上の接続数を削減できるが，
内部的にはRC系の接続状態管理に依存しており，状態管理コストを根本的に消すものではない．
そのため，DCはRCのスケーラビリティ問題を緩和する有力な手段ではあるが，完全な解決策としては扱えない点に注意が必要である．

\begin{table}[t]
	\caption{RDMAトランスポートの比較（スケーラビリティ観点）}
	\label{tab:rdma-transports}
	\centering
	\begin{tabular}{l|p{0.22\linewidth}|p{0.24\linewidth}|p{0.26\linewidth}}
		\hline
		方式 & 主な利点 & 主な制約 & スケーラビリティ上の論点 \\
		\hline
		RC & 信頼性・順序性・one-sided通信が利用可能 & 接続ごとの状態をNIC/メモリに保持 & 接続数増加で状態量とキャッシュ競合が増えやすい  \\ \hline
		UD & 接続レスで状態量を抑えやすい & 信頼性・順序制御・再送をソフトウェアで実装 & 大規模化しやすい一方，ミドルウェア設計負担が増加  \\ \hline
		DC & 見かけ上の接続数を圧縮しやすい & 実装複雑性と切替コスト，機能依存性 & RC状態管理の緩和は可能だが根本解消ではない  \\
		\hline
	\end{tabular}
\end{table}

\section{関連研究}

\subsection{FaRM，eRPC，ScaleRPC}

ソフトウェアによるRDMA RPCのスケーラビリティ改善は，主に「接続状態の扱い」「受信処理の検出コスト」「メモリ階層競合の抑制」という3つの観点で進展してきた．
FaRMはone-sided RDMA read/writeを中核に据え，RDMA writeベースのメッセージングをリングバッファとポーリングで実装した\cite{dragojevicFaRMFastRemote2014}．
同研究は高い性能を示す一方で，チャネル数増加に伴うポーリング負荷と，接続増加に伴うNIC上のQueue Pair状態キャッシュ不足を報告しており，
対策として(1)接続粒度をthread-to-threadからthread-to-nodeへ粗粒化し，(2)NUMAを意識したQP共有を導入して状態量を削減している\cite{dragojevicFaRMFastRemote2014}．
すなわちFaRMのアプローチは，RCベースの高速性を維持しつつ，接続の束ね方をソフトウェアで最適化する方向である．
これに対し本研究は，スレッド単位ではなくノード内の複数プロセス単位で通信を集約する点で異なる．

eRPCは，接続状態をNICに保持するRC系設計そのものをスケーラビリティ制約と捉え，packet I/O（UDP/UD相当）を用いてNIC管理状態を削減する設計を採る\cite{kalia2019DatacenterRPCsCan}．
論文中では，RDMA接続数が増えるとNIC SRAM上の接続状態キャッシュ制約により性能が低下することを示し，eRPCでは接続状態をCPU側で管理することで大規模接続時の性能維持を図っている\cite{kalia2019DatacenterRPCsCan}．
加えて，event loopによるdispatch処理，session creditによるエンドツーエンドフロー制御，zero-copy送受信を組み合わせることで，
汎用RPCとしての機能性と高スループットを両立している\cite{kalia2019DatacenterRPCsCan}．
UD系アプローチで性能が伸びる主要因は，(1)NIC側の接続状態保持を削減できること，(2)受信検出をCQベースで定数時間化しやすいこと，(3)接続切替に伴うNICキャッシュミスを避けやすいことである\cite{kalia2019DatacenterRPCsCan}．
一方で，信頼性制御や再送制御，および大きなデータ転送時のバッファ管理をソフトウェア側で担う必要があり，ワークロードによっては設計複雑性が増す．

ScaleRPCは，RCを維持したままスケーラビリティを高めるため，接続を時間分割で扱うconnection groupingと，
物理メッセージプールを論理的に多重化するvirtualized mappingを提案している\cite{chenScalableRDMARPC2019}．
前者は同時にアクティブ化する接続群を制限してNICキャッシュのスラッシングを抑え，後者はメッセージバッファ共有によってCPU最終段キャッシュ競合とメモリ使用量を削減する\cite{chenScalableRDMARPC2019}．
さらに，優先度ベースのスケジューリングとwarmup機構によりグループ切替オーバーヘッドを隠蔽し，RCの利点を保ちながら大規模接続への適応を図っている\cite{chenScalableRDMARPC2019}．

これら3研究は，いずれもソフトウェアでの工夫によりRDMA RPCのスケーラビリティを改善しているが，設計思想は異なる．
FaRMとScaleRPCはRCを前提に接続状態管理の効率化を狙うのに対し，eRPCはNIC接続状態依存を減らす方向で拡張性を確保する．
また，これらの多くはデータセンター内KVS/DB系ワークロードを主対象としており，要求メッセージが小さく制御可能である前提が強い．
HPC向けad-hocファイルシステムでは，計算ノード内でクライアントとストレージデーモンが同居し，大量同時アクセスと大きなデータ転送が混在するため，
サーバ側だけでなくクライアント側を含む通信資源管理が重要になる．
FaRMの最適化単位はスレッド中心であり，eRPCは汎用性を重視する代わりにone-sided操作前提のデータ転送管理を直接提供しないため，
HPCストレージ向け実装では追加の設計判断が必要となる．
本研究はad-hocファイルシステムを想定し，RCの機能を利用しつつノード内集約で接続状態とメモリ消費を削減する点で，RC維持型アプローチをHPCストレージ文脈に適用する位置づけとなる．

\subsection{mRPC}

\begin{acknowledgment}
本研究の一部は，JSPS科研費JP22H00509，
NEDO（国立研究開発法人新エネルギー・産業技術総合開発機構）の委託事業「ポスト5G情報通信システム基盤強化研究開発事業」（JPNP20017），
JST CREST JPMJCR24R4，
筑波大学計算科学研究センター学際共同利用プログラム，および富士通との特別共同研究の結果得られたものです．
\end{acknowledgment}

\bibliographystyle{ipsjsort}
\bibliography{filesystem}

\end{document}

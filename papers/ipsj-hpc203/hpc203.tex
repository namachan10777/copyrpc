%%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,eabstractは任意)
%% [noauthor]
%%

%\documentclass[submit,techrep]{ipsj}
\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage[dvips]{graphicx}
\usepackage{latexsym}

\begin{document}

\title{LocustaRPC: 次世代リーダーシップマシンのための\\スケーラブルなRPC基盤}

\etitle{LocustaRPC: A Scalable RPC Framework\\for Next-Generation Leadership Machines}

\affiliate{UT}{筑波大学\\
University of Tsukuba}

\author{中野 将生}{Masaki Nakano}{UT}
\author{前田 宗則}{Munenori Maeda}{UT}
\author{建部 修見}{Osamu Tatebe}{UT}

\begin{abstract}
次世代リーダーシップマシンでは数十万ノード規模のシステムが想定されており，
従来のRPC基盤ではスケーラビリティが課題となる．
本稿では，大規模システム向けのスケーラブルなRPC基盤であるLocustaRPCを提案する．
LocustaRPCは，RDMAを活用した低遅延通信と，
スケーラブルなコネクション管理機構を備える．
評価の結果，既存手法と比較して高いスケーラビリティを達成することを確認した．
\end{abstract}

\maketitle

\section{はじめに}

\section{関連研究}

\section{設計}

現代のHPCノードは、富岳NEXT、Sirius、El capitanといったようにノード内並列性を向上させつつノード数自体は抑えるFat node設計が増えてきている。
ムーアの法則が半導体プロセスの改善とマイクロアーキテクチャ改良によるクロック、IPCの向上よりも、ノード内並列性向上の寄与が大きくなり変質してきたことがその理由である。
そのため、現代的な設計ではノード数自体はむしろ減少し、ネットワークトポロジとしては高い性能が標榜されている一方、
通信ライブラリにおいては未だ個別のクライアントがNICを直接利用する設計が多く、ここにギャップが存在する。
% RDMAXとかのHWによるスケーラビリティと比較して検討する

そこで、本研究においては、計算ノード上でのミドルウェア展開という枠組みを踏まえ、以下を目的としたミドルウェア設計の提案を行う。

% sfence集約は具体的すぎる

\begin{enumerate}
    \item Daemonへのリクエスト集約
    \item Daemonへのリクエスト移譲による接続数節約と、それによるメモリ使用量・スループットの改善
    \item 通信のメモリアクセスパイプライン最適化と、sfence集約によるIPC向上
\end{enumerate}


\section{提案手法}

\subsection{トポロジ設計}

富岳NEXTを例にとると、ノード数としては3400ノード規模であるものの、プロセス数は数百万のオーダーと見積もられている。
ノード間接続3400自体を単純にサポートするのは現実的である一方で、単純にクライアントがサーバ一つに数百万接続というのは現実的ではない。
そこで、分散第一階層ファイルシステムとして、各ノードに配置されたサービスデーモンを経由してクライアントが他ノードのデーモンと通信するアーキテクチャを導入する。
ノード内通信では、L3 store + L3 loadもしくは、キャッシュコヒーレンシプロトコルによるL1-L1転送のおよそ100 ns程度のレイテンシで通信が可能である。
一方で、ノード間通信ではInfiniband latencyでの3 us程度のレイテンシがかかるため、ノード内でのリクエスト集約が理想的である。

本アーキテクチャにおいては、レイテンシ最適化を行いつつも10 MRPS/daemon以上のスループットを出せるクライアント-デーモン通信と、
Queue depthに対してスケーラブルなDaemon-Daemon間のRPC層を用いて、デーモンがリクエストを集約することで、
Fat nodeによる大規模環境においても安定的かつ柔軟なミドルウェア通信が行えるアーキテクチャを提案する。

また、ノード間接続を最小化するため、ノード間での接続をデーモンでシャーディングする。クライアントはローカルの中継デーモンをクライアントがわで計算してそのローカルデーモンへ直接リクエストを送り、
受け取ったデーモンが代理で外部のデーモンとの通信を行う。
処理担当部分について、自分のコアとは異なるコアが担当するリクエストを受け取った場合は、
最適化されたSPSCベースの処理移譲機構による、in-procedure callでリクエストを処理させ、返答を元にRPCで返答を行う。

これらは小メッセージ向けの最適化パスだが、ファイルシステムでの活用となるとバッファ転送もまた必要である。
小メッセージではリクエストにおける加工される領域が多いこと、コピーコストが十分に小さいことからZero-copyよりもキャッシュコヒーレンシを重視した設計を行う。
これはコア間転送においてはキャッシュライン単位での読み書きでないとfalse sharingが起きて性能が低下することから、
Store bufferが詰まらない限りにおいて64Bまではコピーでも十分効率的であることが理由である。
バッファ転送については、MB単位でのアクセスがあり得るため、コピーコストが大きくなる。
そこで、コピーを最小化するため、クライアントとサーバの/dev/shmによるメモリ共有領域をデーモン側で直接MemoryRegion登録し、
デーモンがそのMemoryRegionに対するRDMAを行うことで大規模メモリ転送をNIC HWに委任する。
これにより、デーモンを中継させたとしてもクライアントへのバッファ転送へメモリコピーを行う必要がなくなる。

\subsection{RPC設計}

通常POSIXインターフェースでは同期的アクセスとなるので、1 node中のリクエストを集約してもQueue depthはあまり稼げない。
% できれば: 既存のHPCアプリでの命令をカウントしておく
重要であるのは、RQ/SQをリソース上のボトルネックから外してRNR retryを防ぐこと、レイテンシを最小化しつつスループットを向上させることである。
% 関連研究を引く。多分one-sided擁護が多いので困らないはず。ただし、これらはibverbsのオーバーヘッドが原因というのは注意
設計上でノード内リクエストの集約によって接続数を絞るので、N-to-N通信ではなく、1-to-1通信をノードごとに用意し、end-to-endのflow controlによって確実な通信を行う。
3000ノード級に対応するため、CQEにイベントを集約してpollingコストを減らす。
従来の研究ではバッファ上にvalidフラグをおいてそこをpollingする手法が提案されていたが、これはibverbsのAPIがthread safetyを提供するために各種オーバーヘッドが大きかったためであり、
CQEもバッファも同じくNICが書き換えるメモリのPollingに過ぎず、現代のアーキテクチャにおいてはMESIFの最適化により書き換えは重大なボトルネックとはならない。
% 従来研究のサーベイ論文があったのでそれを引く。で、CQEに対するDDIOを分析。あとFast-ForwardでのI/E遷移の話もしよう。
% I/E遷移より実はL3→L1昇格の方が若干だけ速いってのも重要な話
% 出来れば比較をする
むしろ、ポーリング領域を一本に集約し、NIC HWに通知集約をオフロードすることによる性能向上が重要である。

そこで、バッファ管理コストを低減し高速な通信を行うため、WRITE with Immediateを中心に設計する。

call/responseのみをモデルとしてサポートし、送信リング、受信リングでのバッファ間転送としてRPCを実現する。
逆方向の通信でも同様にリングを使うので、それぞれエンドポイントは対向あたり送受信2つの固定長リングバッファを持つ。
受信はWrite with ImmediateのImmediate通知に集約し、Immediate 32 bitをEndpoint IDとする。
また、それぞれのbulk messageの先頭には単調増加するPiggybacked seq, message countをもち、個別のメッセージについては、
len, payloadを持つ。受信した際はImmediateでEndpoint IDを直接特定し、
先頭を読んでPiggybacked seqでフロー管理を更新し、それ以降の受信メッセージをmessage countに達するまで読み込む。

seqは受信して上書き可能になったメッセージの容量であり、バッファはメッセージごとに64B alignされているので64 B blockの数として表現される。
このSeqはそれぞれのバッファリングされた返信の書き込みの際に、最新の値を先頭に入れることでFlow controlを行う。
ただし、これではデッドロックに陥るケースが存在する。
Seqの通知を受けない限りバッファ解放を知れないため通信が出来ないが、Seqを送るには対向側に空きが存在する必要がある。
そこで、リクエストごとにレスポンス許容量を設定する。このレスポンス許容量と、現在の受信バッファの空きを見て、リクエストを送れるかを判断する。

まずReqとして送れるInflight Buffer blockの数を制限する。これは設定可能だが、デフォルトではリングの半分とする。
まずReqを送る際に、このReq用のInflight容量を超えていないかをチェックする。超えていればRing fullでエラー。
これに加えて、InflightなReqに設定されたResponse許容量の合計から、Responseの容量予約が取れるかを確認する。
取れなければここでもエラー。これにより、厳密なFlow controlの進行性を保証する。

これらの処理については、送信API単体では実際には送信されず、poll()の呼び出しによって進行されるようにすることでバッチングを行い、
NIC負荷を下げてQueue depthを活用する設計としている。

\subsection{Client-Daemon通信}

Client-Daemon通信はx86\_64のTSOを活用し、SPSCでOut-of-Order返答が可能なリングを用いる。
クライアントごとにLaneを分離し、サーバ側が全てをPollingすることでクライアント調停コストを最小化する。
また、Laneに対して連続的に多重化されたリクエストをDrainすることで性能を稼ぐ。

\section{実装}

\section{評価}

\section{おわりに}

\begin{acknowledgment}
謝辞をここに記述する．
\end{acknowledgment}

\bibliographystyle{ipsjsort}
\bibliography{filesystem}

\end{document}

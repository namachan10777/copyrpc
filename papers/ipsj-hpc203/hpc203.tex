%%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,eabstractは任意)
%% [noauthor]
%%

%\documentclass[submit,techrep]{ipsj}
\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage[dvips]{graphicx}
\usepackage{latexsym}
\usepackage{url}

% ipsjクラスの\doiがDOI中のアンダースコアを処理できない問題を回避
\makeatletter
\renewcommand{\doi}[1]{\url{https://doi.org/#1}}
\makeatother

\begin{document}

\title{LocustaRPC: 次世代リーダーシップマシンのための\\スケーラブルなRPC基盤}

\etitle{LocustaRPC: A Scalable RPC Framework\\for Next-Generation Leadership Machines}

\affiliate{UT}{筑波大学}
\affiliate{CCS}{筑波大学計算科学研究センター}
\affiliate{FUJITSU}{富士通株式会社}

\author{中野 将生}{Masaki Nakano}{UT}
\author{前田 宗則}{Munenori Maeda}{CCS,FUJITSU}[maeda.munenori@fujitsu.com]
\author{建部 修見}{Osamu Tatebe}{CCS}

\begin{abstract}
次世代リーダーシップマシンでは数十万ノード規模のシステムが想定されており，
従来のRPC基盤ではスケーラビリティが課題となる．
本稿では，大規模システム向けのスケーラブルなRPC基盤であるLocustaRPCを提案する．
LocustaRPCは，RDMAを活用した低遅延通信と，
スケーラブルなコネクション管理機構を備える．
評価の結果，既存手法と比較して高いスケーラビリティを達成することを確認した．
\end{abstract}

\maketitle

\section{はじめに}

現代のHPCシステムでは、富岳NEXT、Sirius、El Capitanに代表されるように、
ノード内並列性を大幅に向上させたFat node設計が主流となりつつある。
この傾向により、ノード数は数千規模に抑えられる一方で、ノードあたりのプロセス数は数百から数千に及び、
システム全体のプロセス数は数百万のオーダーに達する。

こうした環境において、HPCミドルウェア、特にad-hocファイルシステムやキーバリューストアの
RPC基盤にはスケーラビリティの課題が生じる。
まず第一に、従来のRPC設計では個々のクライアントがNICを直接利用するため、
プロセス数の増大に伴い接続数が爆発的に増加し、NICリソースの枯渇やメモリ消費の増大を招く。
また、通信層が十分な性能を発揮しただけでは、全プロセスのリクエストが集中するシナリオでの負荷問題が残るため、
ハードウェアでの解決では固有の負荷軽減策を差し込むことが難しい。

そこで本研究では、ノード内IPC、プロセス内デリゲーション、ノード間RDMA通信の全ての通信層を
メモリアクセス最適化問題として統一的に扱うHPCの第一階層ストレージを主眼としたRPC基盤、LocustaRPCを提案する。
LocustaRPCは、ノードごとに配置されたサービスデーモンがリクエストを集約し、
接続数を削減しつつ少数通信経路への集約を行うことで、従来課題であった大規模ワークロードでの通信の安定性とスケーラビリティを改善し、
また、Queue depthに着目した最適化により性能低下を抑えることが可能であることを示す。

本稿の貢献は以下の通りである。
\begin{enumerate}
    \item ノード内通信中継でのコネクション削減による大規模環境への適応
    \item end-to-end flow controlによる、リクエスト消失の防止を行うRPCの提案
    \item ノード内通信/RPCの協調最適化
\end{enumerate}

\section{関連研究}

\subsection{WRITE系RDMA RPCとデータ配置・到着通知}

RDMA WRITEを用いたRPCは、送信側がone-sided WRITEで受信側メモリへ要求・応答を直接配置し、
受信側CPUが到着を検知して処理する系統であり、
データ配置と到着通知の設計が性能を左右する。

メモリポーリング型の代表であるFaRM\cite{dragojevicFaRMFastRemote2014}は、
受信側のcircular bufferにRDMA WRITEでメッセージを書き込み、
受信側がHead位置をポーリングして非ゼロ値により到着を検知する。
しかしチャネル数増加に伴いポーリング対象が線形に増加する問題があり、
FaRM自身が大規模化時にはWRITE with Immediate Data（WRITE+IMM）とShared Receive Queue（SRQ）による
ポーリングコストの一定化が望ましいことを示唆している。

HERD\cite{kalia2014UsingRDMAEfficiently}はRDMA WRITEで要求を書き込み、
サーバCPUがメモリをポーリングして検知するが、
outbound WRITEが接続数でスケールしない問題を回避するため応答はUD SENDで返すハイブリッド設計を採る。

研究史的にはCQポーリングはibverbs経由のspin lockやコピーのオーバーヘッドにより高コストとして敬遠されてきた。
Fentら\cite{fent2020LowLatencyCommunicationFast}も
WRITE+IMMとメモリポーリングを比較検討し、
CQポーリングが無条件に有利でないことを可視化している。

本研究は、ibverbsをバイパスしたCQハードウェアバッファへの直接アクセスと
複数QPのCQイベント集約により、CQポーリングの再評価を行う。
これはFaRMが示唆した「WRITE+IMM+SRQによるポーリングコスト一定化」を具体的に実装・体系化したものと位置づけられる。

\subsection{RDMA RPCのフロー制御とプロトコル設計}

FaSST\cite{kaliaFaSSTFastScalable2016}、eRPC\cite{kaliaDatacenterRPCsCan2019}は汎用的かつ高速なRPCフレームワークであり、
session creditsによるend-to-endフロー制御でスイッチキューイングを抑制する。
本研究のcredit\_grant/resp\_reservation設計は同じcredit系譜にあるが、
eRPCはUD上の設計であり、本研究ではRCの信頼性をHWにオフロードし、Flowのみをコントロールする設計に比べて
負荷が大きい。また、ファイルシステムのような大容量通信を行う場合はやはりRC接続ないしDC接続が必要であり、
eRPC経路に加えて他の経路が必要になってくるという問題がある。

RFP\cite{wuRFRPCRemoteFetching2019}は、
サーバのoutbound RDMA発行コストが高くinbound/outboundに非対称性があること、
server-bypassが複数回の追加RDMAを要しがちであることを分析し、
応答をクライアントがREADで取りに行くパラダイムを提案した。
本研究は応答もWRITEで返す設計ではあるものの、
HPCの第一階層ストレージにおいては、計算プロセス自体のCPU資源は余るものの
ストレージデーモンは相互通信を行い、かつREADは同時受入可能数の制限があることから大規模環境ではスケーラビリティの課題がある。

%HatRPC\cite{li2021HatRPCHintacceleratedThrift}は、
%payloadサイズ・並行度・性能目標に基づき、
%Direct-WriteIMM、RFP、Send/Recv等の複数RDMAプロトコルを切り替える設計空間を提示し、
%WRITE+IMMが常に最適ではないことを示した。
%本研究はWRITE+IMM中心に設計を収束させているため、
%勝つ条件領域（小〜中メッセージ、低レイテンシ重視、lossless前提）を明確に評価する。

%Weiら\cite{wei2018DeconstructingRDMAenabledDistributed}は
%分散トランザクション文脈で、one-sided/two-sidedのどちらか一方が全フェーズに勝つわけではないことを
%phase-by-phaseで示し、ハイブリッドが最適と結論した。

\subsection{接続スケーラビリティ}

RC QPの状態爆発（RNIC SRAM不足によるPCIe read増加と性能低下）は、
2020年代のRDMA研究で最も重要な争点の一つである\cite{kong2023UnderstandingRDMAMicroarchitecture}。

ScaleRPC\cite{chenScalableRDMARPC2019}はconnection groupingで同時アクティブ接続数を抑え
NICキャッシュスラッシングを緩和し、virtualized mappingでメッセージプール共有によりCPUキャッシュ競合を抑制する。
ただし、グルーピングに起因するレイテンシ分布の二峰性というトレードオフがある。
また、確実なグループ切り替えのため、完全なパイプライン化が難しくスループットが伸びづらく、実装も複雑である。


Flock\cite{monga2021BirdsFeatherFlock}はconnection handle abstractionで
スレッドとQPを多対多に多重化し、coalescingベースの同期とcredit renewalでスケーラビリティを改善した。
本研究ではデーモンが通信集約を行う点でFlockと類似しているが、本研究では異なるプロセスからの通信の中継を可能としている。

RDMAX\cite{ma2025RdmaxScalableRDMA}はRC QP multiplexingで
1つのRC QPに複数クライアントを束ね、
in-network request dispatchingで衝突のない書込み位置決定をネットワーク側へオフロードする。
この研究ではスケーラビリティの高い改善が見られるが、プログラマブルスイッチが必要となることから既存のHPCシステムへの適用は難しい。

SRC\cite{zhao2025SRCScalableReliable}はQPと``接続''の結合自体を問題視し、
QPとconnectionsをdecoupleすることで状態量を桁違いに削減する。
512ノード規模で、RCの146\,MB/nodeに対しSRCは0.19\,MB/nodeを報告している。
将来的なスケーラビリティ改善が期待されるものの、ハードウェア改良であるため既存のHPCシステムへの適用は難しい。

SRNIC\cite{wang2023SRNICScalableArchitecture}はRNICのon-chip状態をモデルに基づき分解し、
プロトコル・アーキテクチャ共同設計で10K接続規模を性能劣化なしに達成した。
StaR\cite{wang2021StaRBreakingScalability}は高並行側のRNICをstateless化により軽量化し、
1RMA\cite{singhvi20201RMAReenvisioningRemote}はマルチテナントDC制約を踏まえ
one-sided primitives中心で send/recvはソフトウェアで代替する再設計思想を示した。

本研究のCQ集約とSRQ共有は「受信側資源管理の定数化」としてSRC等の共有化潮流と整合するが、
RC QPを接続ごとに保持する限りQP状態量問題は残る。
本研究ではデーモン中継による接続集約で
QP数そのものを物理的に削減するアーキテクチャ的回答を採用しており、
ソフトウェアQP共有（Flock）やグルーピング（ScaleRPC）とは異なる比較軸を提供する。

\subsection{RDMA設計指針とNICマイクロアーキテクチャ}

Kaliaらは高性能RDMAシステムの設計指針として、doorbell batching、DDIO、NUMA配置の重要性を示した\cite{kalia2016DesignGuidelinesHigh}。
本研究はこれらの知見をノード内通信からノード間通信まで、スタック全体に拡張して適用する。

Kongら\cite{kong2023UnderstandingRDMAMicroarchitecture}はRDMA NICのマイクロアーキテクチャリソース（QPコンテキストキャッシュ等）を定量的に分析し、
QP数増加時の性能低下メカニズムを明らかにした。
本研究では、Connect-X7においてはQP数1000程度まで実用的な性能が維持されることを踏まえ、
アーキテクチャレベルでの接続集約によりQP数を管理可能な範囲に抑える設計を採用する。

\subsection{ノード内通信とデリゲーション}

Calciuら\cite{calciuMessagePassingShared2013}はマルチコア環境における
メッセージパッシングと共有メモリの性能を定量的に比較し、
デリゲーションの有効性と限界を示した。
本研究はこの知見を踏まえ、SPSCベースのデリゲーション機構を採用しつつ、
キャッシュコヒーレンシプロトコルの影響を最小化するメモリアクセス設計を行う。

mRPC\cite{chen2023RemoteProcedureCall}はRPCのmarshallingとポリシーを
システムサービスとして統合し、サイドカー比で大幅な高速化を報告した。
本研究は通信基盤の極限最適化に重心を置くが、
mRPCが示す``RPCのサービス化''の方向性は、
デーモン中継アーキテクチャと相補的な位置づけにある。

\section{設計}

現代のHPCノードは、富岳NEXT、Sirius、El capitanといったようにノード内並列性を向上させつつノード数自体は抑えるFat node設計が増えてきている。
ムーアの法則が半導体プロセスの改善とマイクロアーキテクチャ改良によるクロック、IPCの向上よりも、ノード内並列性向上の寄与が大きくなり変質してきたことがその理由である。
そのため、現代的な設計ではノード数自体はむしろ減少し、ネットワークトポロジとしては高い性能が標榜されている一方、
通信ライブラリにおいては未だ個別のクライアントがNICを直接利用する設計が多く、ここにギャップが存在する。
% RDMAXとかのHWによるスケーラビリティと比較して検討する

以降、これら3つの目標を達成するために、メモリアクセス最適化とQueue depthを軸とした提案手法を述べる。

\section{提案手法}

\subsection{トポロジ設計}

富岳NEXTを例にとると、ノード数としては3400ノード規模であるものの、プロセス数は数百万のオーダーと見積もられている。
ノード間接続3400自体を単純にサポートするのは現実的である一方で、単純にクライアントがサーバ一つに数百万接続というのは現実的ではない。
そこで、分散第一階層ファイルシステムとして、各ノードに配置されたサービスデーモンを経由してクライアントが他ノードのデーモンと通信するアーキテクチャを導入する。
ノード内通信では、L3 store + L3 loadもしくは、キャッシュコヒーレンシプロトコルによるL1-L1転送のおよそ100 ns程度のレイテンシで通信が可能である。
一方で、ノード間通信ではInfiniband latencyでの3 us程度のレイテンシがかかるため、ノード内でのリクエスト集約が理想的である。

本アーキテクチャにおいては、レイテンシ最適化を行いつつも10 MRPS/daemon以上のスループットを出せるクライアント-デーモン通信と、
Queue depthに対してスケーラブルなDaemon-Daemon間のRPC層を用いて、デーモンがリクエストを集約することで、
Fat nodeによる大規模環境においても安定的かつ柔軟なミドルウェア通信が行えるアーキテクチャを提案する。

また、ノード間接続を最小化するため、ノード間での接続をデーモンでシャーディングする。クライアントはローカルの中継デーモンをクライアントがわで計算してそのローカルデーモンへ直接リクエストを送り、
受け取ったデーモンが代理で外部のデーモンとの通信を行う。
処理担当部分について、自分のコアとは異なるコアが担当するリクエストを受け取った場合は、
最適化されたSPSCベースの処理移譲機構による、in-procedure callでリクエストを処理させ、返答を元にRPCで返答を行う。

これらは小メッセージ向けの最適化パスだが、ファイルシステムでの活用となるとバッファ転送もまた必要である。
小メッセージではリクエストにおける加工される領域が多いこと、コピーコストが十分に小さいことからZero-copyよりもキャッシュコヒーレンシを重視した設計を行う。
これはコア間転送においてはキャッシュライン単位での読み書きでないとfalse sharingが起きて性能が低下することから、
Store bufferが詰まらない限りにおいて64Bまではコピーでも十分効率的であることが理由である。
バッファ転送については、MB単位でのアクセスがあり得るため、コピーコストが大きくなる。
そこで、コピーを最小化するため、クライアントとサーバの/dev/shmによるメモリ共有領域をデーモン側で直接MemoryRegion登録し、
デーモンがそのMemoryRegionに対するRDMAを行うことで大規模メモリ転送をNIC HWに委任する。
これにより、デーモンを中継させたとしてもクライアントへのバッファ転送へメモリコピーを行う必要がなくなる。

\subsection{RPC設計}

本節では、ノード間RPC層の設計を述べる。
設計上の主要な目標は、(1)~RQ/SQをリソース上のボトルネックから外してRNR retryを防ぐこと、
(2)~レイテンシを可能な限り最小化しつつスループットを向上させること、
(3)~3000ノード級での受信ポーリングのスケーラビリティを確保することの3点である。

デーモン中継によりノード内リクエストが集約されるため、
ノード間通信はN-to-N通信ではなくノードペアごとの1-to-1通信となる。
各エンドポイントは対向に対して送受信2つの固定長リングバッファを保持し、
end-to-endのクレジットベースフロー制御により確実な通信を行う。

\subsubsection{プリミティブの選択}

RDMA WRITE with Immediate Data（WRITE+IMM）を通信プリミティブとして採用する。
WRITE+IMMは、送信側がRDMA WRITEでデータを受信側リングに直接配置すると同時に、
32ビットのImmediate値により受信側CQにCompletion Queue Entry（CQE）を生成する。
これにより、データ配置と到着通知が単一の操作で完結する。

既存研究においてはCQポーリングは負荷が大きいとする認識があったが、
mlx5ドライバのメモリを直接扱うことにより、CQポーリングもバッファ上のフラグポーリングと同等のコストまで低下させることが可能である。
本質的にはバッファ上のフラグのポーリングであっても、CPUで直列化する必要があるが、CQポーリングによりNICのCQ出力にこの直列化コストをオフロードする
ことが可能となる。

メモリポーリング型の設計\cite{dragojevicFaRMFastRemote2014,kalia2014UsingRDMAEfficiently}では
FaRM\cite{dragojevicFaRMFastRemote2014}自身もWRITE+IMMとSRQによるポーリングコスト一定化を示唆しており、
本設計はこれを具体的に実装・体系化したものである。

また、CQはWRITE完了時点で発生するため、validフラグのクリアが完全に不要となるのも利点である。
従来の研究ではibverbs経由のCQポーリングが高コストとして敬遠されてきた\cite{fent2020LowLatencyCommunicationFast}。
しかしこれはibverbs実装のspin lock・コピー・間接呼び出しに起因するオーバーヘッドであり、
\ref{sec:ibverbs-bypass}節で述べるibverbsバイパスにより排除できる。

\subsubsection{ワイヤフォーマットとバッチレイアウト}

1回のRDMA WRITE+IMMにより、対向の受信リング上に以下のバッチが書き込まれる。
リクエスト発行では、この送信リングへとリクエストを書き込み、任意のタイミングでバッチをまとめて送信することでQueue depthを活用する。
これにより、高負荷時にはリクエストあたりのWQE発行数を減らすことができ、これは受信側のWQEを減らしWQE枯渇を防止する。

バッチ先頭の32バイトはフローメタデータであり、
送信側の受信リング消費位置\texttt{consumer\_pos}（8バイト）、
対向に付与するクレジット\texttt{credit\_grant}（8バイト）、
後続メッセージ数\texttt{message\_count}（4バイト）、
およびパディング（12バイト）で構成される。
\texttt{consumer\_pos}と\texttt{credit\_grant}はフロー制御に使用され、
全バッチに常にpiggybackされるため、専用のACKメッセージを必要としない。

フローメタデータに続いて、個々のメッセージが連結される。
各メッセージは12バイトのヘッダ（\texttt{call\_id} 4バイト、\texttt{len} 4バイト）とペイロードから成り、
32バイト境界にパディングされる。
\texttt{call\_id}の最上位ビット（RESPONSE\_FLAG）でリクエストとレスポンスを識別する。
リクエスト時の\texttt{piggyback}フィールドにはレスポンス許容量を32バイトブロック単位で格納する。

Immediate値にはバッチ全体のバイト数を32で除した値をエンコードする。
これにより32ビットで最大128GBのバッチサイズを表現でき、
受信側はImmediate値のデコードだけで受信リングのプロデューサ位置を更新できる。

32バイトアラインメントは、キャッシュライン（64バイト）の半分に相当し、
受信側でのメッセージ境界の計算を単純なビットマスク操作で行えるようにする。

\subsubsection{遅延バッチングとリング管理}

RPC呼び出し（call）および応答（reply）は、即座にRDMA送信を行わない。
ローカルの送信リングにメッセージを書き込み、プロデューサ位置を更新するのみである。
実際のRDMA送信は、poll()内のflush処理により、
蓄積された複数メッセージを1回のRDMA WRITE+IMMとして一括送信する。
これにより、WQE消費とdoorbell発行を複数メッセージ分償却し、
NIC側のSQ処理負荷を軽減しつつqueue depthを活用する。

バッチのライフサイクルは3段階からなる。
(1)~最初のメッセージ追加時に、送信リング上の現在位置にフローメタデータ用の32バイトプレースホルダを予約する。
(2)~call()やreply()がヘッダ・ペイロード・パディングを書き込み、バッチ内メッセージ数をインクリメントする。
(3)~poll()時にフローメタデータを確定し（\texttt{consumer\_pos}、\texttt{credit\_grant}、\texttt{message\_count}を書き込み）、
WQEを発行して送信する。

送信リングは固定長の循環バッファであるため、バッチがリング末尾をまたぐケースを処理する必要がある。
メッセージ追加時に、現在のオフセットからメッセージ終端までがリング境界を越える場合、
現バッチを確定・送信した上で、\texttt{message\_count}に特殊なセンチネル値（$2^{32}-1$）を持つ
ラップマーカーを発行し、プロデューサ位置をリング先頭に巻き戻す。
これにより、個々のバッチのScatter/Gather Entry（SGE）がリング境界をまたがないことを保証し、
WQE構築を単純化する。
ラップマーカーにもフローメタデータ（\texttt{credit\_grant}等）が含まれるため、
クレジット情報が失われることはない。

\subsubsection{クレジットベースのレスポンス予約}

送信リング上の消費位置（\texttt{consumer\_pos}）のpiggybackのみではデッドロックが発生しうる。
双方がリクエストでリングを埋め尽くすと、レスポンスを書き込む空きがなくなり、
\texttt{consumer\_pos}を伝播するバッチ自体を送信できなくなる。

この問題を解決するため、クレジットベースのレスポンス予約機構を導入する。
リング容量を$C$ブロックとし、各リングの書き込み側が以下の3量を管理する。
\begin{itemize}
  \item $r$: 対向に未消費のリクエストブロック数
  \item $s$: 対向に未消費のレスポンスブロック数
  \item $R$: レスポンス予約総量（発行済みクレジットから応答書き込み済み分を引いた値）
\end{itemize}
書き込み側は全ての書き込みおよびクレジット発行に先立ち、以下の不変条件を検査する。
\begin{equation}
  r + s + R \leq C \label{eq:invariant}
\end{equation}

クレジットはフローメタデータの\texttt{credit\_grant}フィールドにpiggybackして発行する。
対向はクレジット残高の範囲内でのみリクエストを送信でき、各リクエストにレスポンス許容量$R_i$を設定する。
各操作に対する不変条件の変化を表\ref{tab:invariant}に示す。

\begin{table}[t]
  \caption{各操作に対する不変条件$r + s + R \leq C$の変化}
  \label{tab:invariant}
  \centering
  \begin{tabular}{l|ccc|c}
    \hline
    操作 & $\Delta r$ & $\Delta s$ & $\Delta R$ & 変化量 \\
    \hline
    リクエスト送信 ($q$ブロック) & $+q$ & & & $+q$ ※ \\
    クレジット発行 ($g$ブロック) & & & $+g$ & $+g$ ※ \\
    対向リクエスト受信 ($R_i$) & & & $\pm 0$ & $0$ \\
    レスポンス書き込み ($s_i \leq R_i$) & & $+s_i$ & $-R_i$ & $s_i - R_i \leq 0$ \\
    Seq受信 (消費量$x$) & $-x$ or & $-x$ & & $-x$ \\
    \hline
  \end{tabular}
  \\※ 事前に不変条件を満たすことを確認
\end{table}

対向リクエストの受信時、$R$の内訳は変化する（未使用クレジットが未応答予約に移行する）が総量は不変である。
レスポンス書き込み時は$s$が$s_i$増加し$R$が$R_i$減少するため、変化量$s_i - R_i \leq 0$で不変条件は保たれる。

この不変条件から、レスポンスの書き込み可能性が導かれる。
式(\ref{eq:invariant})よりリングの空き$C - r - s \geq R$が成立する。
$R$は未応答リクエストの許容量$R_i$を含むため$R \geq R_i \geq s_i$である。
よって空き$\geq R \geq R_i \geq s_i$となり、レスポンスの書き込みは常に成功する。

固定的なリング分割と異なり、クレジット発行量を動的に調整することで、
トラフィックパターンに応じたリング利用率の最適化が可能である。
片方向通信時にはリング全体をリクエストに充当でき、
双方向通信時には適応的にレスポンス予約を確保する。

\subsubsection{リクエスト処理順序}

リクエストの取り込みはIn-Orderで行われる。
RC（Reliable Connection）トランスポートはCQEの配信順序を保証するため、
異なるバッチ間でもメッセージの到着順序が維持される。
受信側はバッチ先頭のフローメタデータを読み取ってから後続メッセージを処理するため、
\texttt{consumer\_pos}によるリング消費位置の更新と
\texttt{credit\_grant}によるクレジット付与がメッセージ処理に先行する。
この順序保証は、クレジットベースフロー制御の前提条件である。
また、ラップマーカーの正しい処理にもバッチの到着順序が必要となる。

一方、レスポンスの送信はOut-of-Orderである。
サーバが複数のリクエストを受け取った後、処理完了順に応答を返すことができる。
受信側では\texttt{call\_id}のRESPONSE\_FLAGを検査してレスポンスを識別し、
元の\texttt{call\_id}をインデックスとしてSlabから対応するコールバック情報を直接取得する。
Slabはインデックスアクセスであるため、到着順序に依存しない。
この設計はNVMe Submission Queueのセマンティクスと類似しており、
IOキューとしての順序自由度を確保しつつ、
フロー制御に必要なIn-Order保証を維持する。

reply操作にリングフル検査が不要であることも、
Out-of-Orderレスポンスを安全に実現する要因である。
call時にクレジットが予約済みであり、
不変条件~(\ref{eq:invariant})によりレスポンス分の空きが常に保証されるため、
レスポンスの順序によらずリング枯渇は発生しない。

\subsection{Client-Daemon通信}

Client-Daemon通信はx86\_64のTSOを活用し、SPSCでOut-of-Order返答が可能なリングを用いる。
クライアントごとにLaneを分離し、サーバ側が全てをPollingすることでコア間の調停コストを最小化する。
また、Laneに対して連続的に多重化されたリクエストをDrainすることで性能を稼ぐ。
producer側はstore onlyとし、consumer側はload onlyとする。
メッセージのstore時にgenerationカウンタを付与し、consumerはgenerationカウンタをチェックすることで、どこまでが公開されたメッセージかを判別する。
このアルゴリズムではstore側のoverwriteの危険があるが、通信をcall/responseに限定し、
response到着によって送信可能クレジットを回復することで送信側のみで上書きを行わないフロー制御が可能である。
これは、req/responseの処理順序をAPIレベルでin-orderを強制することで安全性が保障される。
また、x86\_64のメモリモデルはTSOであり、
あるコアでのStore順序は他のコアでも同じ順序で観測される。このアルゴリズムでは基本的に片側は一方的なStoreで、片側は連続的なLoadなので
TSOではAtomicやFenceなしに順序保証がなされる。TSOで許容されるStoreからLoadへのreorderingが発生した場合でも、
観測ずみのReqに対する返信よりも、未観測のReqに対するLoadが先行するのみである。
Responseの発行がOut-of-Orderになることはアルゴリズム上許容されており、
あくまでReq/Resの処理順序はLoad/Loadによって達成されるため一貫性は破壊されない。
Storeが遅れることは単純に処理リクエストの通知が遅れるのみであり、
高負荷時においてスループット低下の可能性が存在するのみとなる。

コア間通信において、キャッシュコヒーレンシプロトコル（MESIFのForward遷移等）によるL1-L1転送は
最も直接的なデータ共有手段である。
しかし、実測ではCLDEMOTE命令によりデータを明示的にL3に降格させ、
他コアがL3から読み出す方がレイテンシが低い場合がある。
これは、Forward遷移に伴うスヌープのオーバーヘッドが、
L3の共有キャッシュとしてのアクセスレイテンシを上回るためと考えられる。

これらの最適化により、Queue depth = 1の場合の同期的通信では他のSOTA SPSCベース実装を超えるスループットが達成可能であり、
またQueue depthが増加した場合でも性能低下を抑えることが可能である。
特に、クライアントは典型的なPOSIXインターフェースを用いることが多いことから、
Queue depth = 1に対する最適化は重要である。

本設計では、SPSC通信においてプロデューサ側でCLDEMOTEを活用し、
コンシューマがL3から効率的に読み出せるようにする。
また、キャッシュライン境界での64Bアラインメントと、Lane分離によるfalse sharingの回避、
SPSC設計によるHITM（Hardware Interrupt To Modify）イベントの削減を行う。

ノード内通信においては固定長スロットを採用しているが、これはノード間ではレイテンシが数μsとなり通信回数自体が性能低下を招くのに対して、
ノード内ではキャッシュライン争奪が主要な性能低下要因であり通信回数自体は性能低下要因ではないことから、
先に可変長部分をメモリに書いた上でポインタのみを転送することでキャッシュライン争奪を避けることが可能であるためである。

\subsection{Daemon-Daemon delegation}

同一プロセス内のDaemonスレッド通信では、複数クライアントからの複数リクエストが集約されるため
Client-Daemonに対してより高いQueue depthが想定される。
極めて高いQueue depthにおいては、lamport styleのアルゴリズムを改良し、head/tail両方キャッシュして定期的な同期とすることで、
head/tailでの同期でstallさせてそれ以降の読み書きを高速化する手法で高いスケーラビリティが達成される。
一方で、lamport-styleではインデックスでのキャッシュラインをコヒーレンシプロトコルで奪い合った上でデータの読み書きを行うため、
Queue depthが低くなると極度に性能が悪化する。
Fast-Forwardingでは、validフラグによって進行を管理してconsumerがinvalidを書き戻すことでproducerとconsumerの間のキャッシュライン争奪を明示的に行い、
consumerが先んじてキャッシュラインを奪っておくことでスループットを最大化可能だが、
提案手法でのone-sided SPSCに対して大きな優位性はなく、
Queue depth = 1の低負荷時のオーバーヘッドが大きいことからこちらでもone-sidedでのSPSCを束ねてMPSCとする手法を採用した。

\subsection{アーキテクチャ}

提案手法では、これらのコンポーネントを用いた上で、
クライアントとDaemonはn-to-nでの全接続を行う。
クライアントはDaemonから事前に提供された手段を用いて適切な中継Daemonを選択しそこにリクエストを送る。
ローカルDaemonで処理不可能な場合は所有するコネクションを用いて他ノードへとリクエストを転送する。
転送されたリモートのDaemonは、自身で処理するか自身の処理範囲ではない場合はSPSCでのDelegationにより担当スレッドへとリクエストを転送する。
返送は逆順で行う。中継数の増加はレイテンシを増加させスループットの低下を招くのでクライアント側で事前にローカルの適切なDaemonへ転送することで
ローカルDaemon内の通信を削減する。

メタデータやコントロール用の小メッセージに対して、バッファ通信ではコピーコストが大きくなることからzero-copyでの通信が重要となる。
そこで、client-daemonやdaemon-daemon delegationではコントロールメッセージのみの通信とし、バッファについてはメモリ情報のみを伝達してNICに通信をオフロードする。
NICはPCIe経由接続であり、コアローカリティの影響を受けない。クライアントはデーモンとの共有メモリ領域を持ち、
クライアント側で管理する空き領域の情報をデーモンへ伝達する。デーモンは共有メモリ領域を事前にNICにMemory region登録し、そこへの書き込みをNICに依頼することで
DMAを用いてSPSCによるDelegationチェーンを外れた転送が可能である。

\section{実装}



\section{評価}

\subsection{評価環境}

評価にはfern03およびfern04を使用する。
各ノードはIntel Xeon Gold 6530（32コア）を搭載し、
NICとしてMellanox Connect-X7をNUMA 0に接続している。
NUMAノードは1個の構成であり、コア0--1はIRQ処理用に予約し、ベンチマークでは使用しない。
ノード間はInfiniBandで接続される。

\subsection{ノード内通信性能}

Client-Daemon通信（IPC）およびデーモン内デリゲーション（inproc）の性能を評価する。

IPC評価では、QD=1でクライアント数を1--32まで変化させ、
SPSC集約方式とMPSC方式のスループット・レイテンシを比較する。
合わせて、perf c2cによるHITMイベント数を計測し、
キャッシュコヒーレンシオーバーヘッドの定量分析を行う。

inproc評価では、QD=8--32でデーモンワーカースレッド間の全対全通信性能を測定し、
学術論文中のSOTA実装との比較を行う。
% TODO: 比較対象の具体的な論文リストを確定

\subsection{RPC性能}

ノード間RPC通信について、以下の3手法を比較する。
\begin{itemize}
    \item copyrpc: 本提案手法
    \item UCX Active Message: UCXフレームワークによるRPC
    \item Naive send/recv: ibverbs上の素朴なsend/recv実装
\end{itemize}

メッセージサイズ（64B, 256B, 1KB, 4KB）ごとのレイテンシとスループットを測定する。
また、QP数を10から3000まで変化させたスケーラビリティ評価と、
バッチサイズによるdoorbell batching効果の分析を行う。

% TODO: 間に合えば memory polling vs CQ pollingの直接比較

\subsection{統合評価}

benchkvを用いたEnd-to-End評価を行う。
benchkvはrank IDとキーによる単純なキーバリューストアであり、
アーキテクチャの評価を主目的とする。

YCSBライクなワークロードとして、read/write比率を
90\%/10\%、50\%/50\%、10\%/90\%に設定した3パターンで評価する。
メトリクスとして、IOPS、レイテンシ（p50, p99）を測定し、
メモリアクセス指標（HITMイベント数、キャッシュミス率）とIOPSの相関を分析する。

マルチノード構成でのスケーラビリティ評価も行い、
デーモン中継アーキテクチャが大規模環境で有効であることを検証する。

\section{おわりに}

本稿では、Fat node環境におけるスケーラブルなRPC基盤LocustaRPCを提案した。
LocustaRPCは、ノード内IPC、デーモン内デリゲーション、ノード間RDMA通信の全てを
メモリアクセス最適化問題として統一的に捉え、
デーモン中継型アーキテクチャにより接続数削減とQueue depth活用を両立する。

ibverbsのバイパスによるCQポーリング効率化、CLDEMOTEを活用したキャッシュ制御、
sfence集約によるStore buffer最適化など、各通信層においてメモリアクセスパターンを
意識した設計を行った。

% TODO: 評価結果のサマリを記述

今後の課題として、マルチNUMAノード環境での評価、
ファイルシステムやデータ分析など多様なワークロードへの適用、
次世代NIC（RDMAX等）のハードウェア支援との統合が挙げられる。

\begin{acknowledgment}
本研究の一部は，JSPS科研費JP22H00509，
NEDO（国立研究開発法人新エネルギー・産業技術総合開発機構）の委託事業「ポスト5G情報通信システム基盤強化研究開発事業」（JPNP20017），
JST CREST JPMJCR24R4，
筑波大学計算科学研究センター学際共同利用プログラム，および富士通との特別共同研究の結果得られたものです．
\end{acknowledgment}

\bibliographystyle{ipsjsort}
\bibliography{filesystem}

\end{document}

%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,eabstractは任意)
%% [noauthor]
%%

%\documentclass[submit,techrep]{ipsj}
\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage[dvipdfmx]{graphicx}
\usepackage{latexsym}
\usepackage{url}

% ipsjクラスの\doiがDOI中のアンダースコアを処理できない問題を回避
\makeatletter
\renewcommand{\doi}[1]{\url{https://doi.org/#1}}
\makeatother

\begin{document}

\title{LocustaRPC: 次世代リーダーシップマシンのための\\スケーラブルなRPC基盤}

\affiliate{UT}{筑波大学大学院情報理工学位プログラム}
\affiliate{CCS}{筑波大学計算科学研究センター}
\affiliate{FUJITSU}{富士通株式会社}

\author{中野 将生}{Masaki Nakano}{UT}[mnakano@hpcs.cs.tsukuba.ac.jp]
\author{前田 宗則}{Munenori Maeda}{CCS,FUJITSU}
\author{建部 修見}{Osamu Tatebe}{CCS}

\begin{abstract}
リーダーシップクラスのHPCマシンでは，総プロセス数が極めて多いことからad-hocファイルシステムのようなミドルウェアにおいて，通信接続状態の増大が性能低下とメモリ消費増大を同時に引き起こす．
本稿では，ノード内プロセスのRPC要求を共有メモリ経由でデーモンへ委譲し，ノード外RDMA通信を集約するRPC基盤LocustaRPCを提案する．
提案方式は，クライアントごとに接続を持つ構成に比べて接続管理状態を削減できるため，通信資源使用効率の改善可能性を示す．
一方で，単純な中継構成では中規模条件でバッチング崩壊が発生し，性能劣化が大きいことも確認した．
この課題に対して，低Queue Depth時の到着率に応じて保持時間を動的調整するAdaptive Batch Hold制御を導入した．
ベンチマーク用Key-Value Store実装での評価では，提案方式は2ノードおよび4ノードでUCXおよびdirect connectionを上回る一方，32ノードでは両方式を下回った．ただし同条件でAdaptive Batch Holdはno batch hold比で最大1.44倍の改善を示した．
\end{abstract}

\maketitle

\section{はじめに}

ムーアの法則はパイプラインの深化やOut-of-Order実行に代表されるマイクロアーキテクチャの改良，半導体製造プロセスの改良の両輪によって進んできていた．
しかし，物理的限界によってシングルコアでの性能向上が困難になったことで，マルチコア化やアクセラレータによる一部処理の特殊ハードウェア化が進み，
HPCにおいてもアクセラレータ，メニーコアといった構成が採用されてきた．近年では，El Capitan，Frontierといったリーダーシップマシンでは
アクセラレータとCPUソケットを複数搭載したFat-nodeと呼ばれる構成が採用されている．
日本においても，筑波大学のSiriusスーパーコンピュータ，富岳NEXTにおいてFat-node構成が採用されており，
ノード数自体は減少しつつも全体として高い並列性を持ち高度な計算能力が実現される．

一方で，こうした構成ではインターコネクトの物理設計としてはスケーラビリティを確保するものの，
ソフトウェア的にはプロセス数自体はむしろ増加するため通信ライブラリでのスケーラビリティについての課題が残る．
富岳NEXTでは100万オーダーでのプロセス数が想定されており，
数値計算においては，MPI等の集団通信最適化研究により高い性能を発揮する\cite{weihangjiangHighPerformanceMPI22004,sugiharaDesignLocalityawareMPIIO2020}．
一方で，ストレージ等のHPCアプリを補助するミドルウェアでは，アクセス先の偏りや通信相手を固定的に仮定しにくく，これらの最適化をそのまま適用することは難しい．
しかし，富岳では天文データ解析，機械学習，大気シミュレーションといった高いストレージ性能を要求するアプリへの回答として，
ストレージミドルウェアであるLLIOが導入され，並列ファイルシステムを補完している\cite{akimotoFileSystemPower,nukariyaHPCAIInitiatives}．
このことは，リーダーシップマシンの実運用においても，計算を補助するミドルウェアの通信スケーラビリティが設計課題として重要であることを示している．

この種の通信層スケーラビリティ課題は，データセンター内データベースの最適化問題として先行研究が蓄積されてきた．
ただし，データセンター内データベースではサーバが少数でクライアントが多数であるのに対して，
HPCアプリにおけるミドルウェアではクライアントもサーバもノード単位では対称な構成となるものも多く，
そのまま適用することは難しい．また，これは性能低下だけではなく通信のためのメモリリソースの増大も発生する．
とりわけHPCアプリでは，計算の補助であるミドルウェアは本体の計算を阻害しないよう少数のCPU，小規模のメモリで動作することが好ましく，
通信の空間的リソースの節約も重要となる．

本研究では，大規模なHPCアプリ実行時に，ad-hocファイルシステムのようなミドルウェアで接続状態の増大により通信性能低下とメモリ消費増大が同時に生じる点を課題とする．
とりわけメモリ消費増大は，HPCノードでOOM killerによる強制終了を誘発し，計算用メモリを直接圧迫するため深刻である．
この課題に対し，共有メモリを用いたノード内通信によってコネクションを集約し，従来のHPC向けRPCより少ないリソースでノード間通信を実現するアーキテクチャを提案する．
さらに，提案アーキテクチャによって検証用Key-Valueストアを実装して，設計の有効性を検証する．
本稿の構成は以下の通りである．
まず，背景でad-hocファイルシステムとRDMA通信における課題を整理する．
次に，設計で提案アーキテクチャと通信プロトコルを示し，関連研究で本稿での提案手法との比較を行う．
実験で簡易的なKey-Value Storeの実装による評価結果を示し，まとめで結論と今後の課題を述べる．

\section{背景}
\label{sec:background}

\subsection{ad-hocファイルシステム}

並列ファイルシステムは，メタデータ管理とデータ管理を分離し，データを複数ストレージノードに分散配置して並列帯域を引き出す構成を基本として発展してきた．
PVFSやLustreに代表される設計は，大規模チェックポイントのような大容量I/Oで高い実効帯域を達成しつつPOSIXファイルシステム操作を高いレベルで実装しており，HPCの標準的な永続ストレージ層として定着している\cite{carnsPVFSParallelFile2000}．
ただし，専用ノードを用いて高性能を出す設計が多く，計算ノード全てを使ったワークロードでは小規模なジョブに比べて相対的に性能劣化が発生する．
また，HPCアプリでは全てのノードで同じタイミングでI/Oが発生するケースが多く，
突発的なI/O集中にはコストのかかる並列ファイルシステムでは対応が難しい．
このギャップは，機械学習やデータ解析ワークロードに見られる小粒度I/Oやメタデータ集中アクセスで特に顕在化し，共有ストレージ単体では安定した処理が難しくなる\cite{brinkmannAdHocFile2020}．

こうした背景から，計算ノード上のNVMe SSDや永続メモリ活用した，一時的な高速大容量ストレージミドルウェアとして，ad-hocファイルシステムが発展してきた．
ad-hocファイルシステムと通常の並列ファイルシステムの比較を\ref{fig:pfs-adhocfs-overview}に示す．
スイッチを挟んで上側が並列ファイルシステムで，下側の計算ノード上で展開されるものがad-hocファイルシステムである．
並列ファイルシステム用のストレージノードは少数であり，かつネットワーク帯域も計算ノードの相互接続用よりも低いため大規模なジョブではad-hocファイルシステムは並列ファイルシステムにハードウェアリソースで勝る．
また，占有するジョブの上で展開されることから，他のユーザによって帯域が奪われない点も安定したHPCアプリの実行に寄与する．
\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/pfs-and-adhocfs.drawio.pdf}
	\caption{並列ファイルシステムとad-hocファイルシステムの構成比較}
	\label{fig:pfs-adhocfs-overview}
\end{figure}

また，CHFSはad-hocファイルシステムを永続メモリ向けに再設計し，専用メタデータサーバを置かず，一貫性ハッシュを用いた分散Key-Value Store上にファイルシステム機能を直接構築することで，
ノード数増加時のメタデータ性能とデータアクセス性能の両方を改善している．
CHFSが示す設計上の要点は，(1)集中管理点の排除，(2)逐次実行パスの削減，(3)ノードローカル媒体の特性に合わせた軽量データ構造であり，
ad-hocファイルシステムのスケーラビリティを議論する上で重要な指針となる\cite{tatebeCHFSParallelConsistent2022}．
さらに，GekkoFS，BurstFS，UnifyFS，DeltaFSに代表される研究も，ワークロード特性に応じてメタデータ管理方式や同期モデルを見直すことで，従来PFSのボトルネックを補完する方向を示している\cite{vefGekkoFSTemporaryDistributed2018,wangBurstFSDistributedBurst2016,brimUnifyFSUserlevelShared2023,zhengDeltaFSExascaleFile2015,zhengDeltaFSScalableNogroundtruth2021}．
杉原らのMPI-IO研究が示すように，既存アプリケーション互換性を維持したままI/O経路の局所性を高める工夫も，引き続き実用上重要である\cite{sugiharaDesignLocalityawareMPIIO2020}．

以上を踏まえると，ad-hocファイルシステムに求められる要件は次の3点に整理できる．
\begin{enumerate}
	\item 既存HPCアプリケーションを活かすためのPOSIX互換インターフェース，またはそれに準ずる透過性
	\item ノード内外で大量に発生する小粒度リクエストを処理するための高いメタデータ性能と低遅延通信
	\item 計算用資源を圧迫しない実装，特に計算ノードメモリ消費を抑えた通信基盤
\end{enumerate}
本研究は，このうち3点目の通信資源効率に焦点を当て，ad-hocファイルシステムの実用スケールでボトルネックとなる接続状態とメモリ消費の問題を扱う．

\subsection{RDMA通信}

RDMAは，CPU介在を抑えつつ低遅延・高帯域のデータ転送を実現する通信機構である．
いくつかの接続形態が存在し，主として利用されるのはReliable Connection（RC），Unreliable Datagram（UD），Dynamically Connected（DC）である．
それぞれの特性を\ref{tab:rdma-transports}にまとめる．

最も広く用いられているのはRCである．
操作に対して一定の順序制御と自動的な再送制御があり，通信管理コストをNICにオフロード可能である．
また，WRITE，READ，ATOMICといった対向のCPUの動作なしにメモリ操作を行うことが可能であり，
特に大容量データを扱うことの多いストレージミドルウェアでは非常に効率的である．
ただし，通信相手に事前にRC接続を張る必要があり，この時の接続情報がNICキャッシュに乗り切らない場合Cache slashingが発生して性能低下が起きることが報告されている \cite{chenScalableRDMARPC2019}．

UDは単一の接続ハンドラのみで多数の相手と通信可能であり，高いスケーラビリティを有している一方で，
順序保証・再送保証なしのSEND/RECVしか使えず信頼性の高い通信をするにはソフトウェアによる制御が必要となる．
特に，READ/WRITEのようなハードウェアオフロード可能な大容量通信が存在しないため，
特にファイルシステムや行列計算では避けられる傾向にある．

以上二つが標準的なibverbsにおける接続形態だが，NVIDIAの独自拡張としてDCが存在している．
これはRCの接続・切断・再接続をNICハードウェアが極めて高速かつ効率的に行う機構に近く，
通信がまばらであったり，特定の相手との通信に時間的に偏る場合はRCと同等の性能を発揮することが可能である．
また，自動的に通信を切ってリソースを抑えるため，1万ノード以上の規模でも任意のノードとの高速な通信を可能にしつつもメモリ使用量を抑えることが可能である．
ただし，複数の相手との同時通信を行う場合，単一の接続ハンドラではRC以上の性能低下が起きることから，
管理コストをハードウェアにオフロード出来る機構としては有用なもののスループットを単独で解決することは難しい．

\begin{table}[t]
	\caption{RDMAトランスポートの比較（スケーラビリティ観点）}
	\label{tab:rdma-transports}
	\centering
	\begin{tabular}{l|p{0.22\linewidth}|p{0.24\linewidth}|p{0.26\linewidth}}
		\hline
		方式 & 主な利点 & 主な制約 & スケーラビリティ上の論点 \\
		\hline
		RC & 信頼性・順序性・one-sided通信が利用可能 & 接続ごとに状態をNIC/メモリに保持 & 接続数増加で状態量とキャッシュ競合が増えやすい  \\ \hline
		UD & 接続数に対してスケール & 低信頼性・順序制御がない・通信命令あたり上限が小さい・one-sided通信は不可能 & 良好なものの，大容量データ転送には向かない．CPU負荷が増大  \\ \hline
		DC & 信頼性・順序性・one-sided通信が利用可能・接続数に対してメモリがスケール & NVIDIA系NICのみ & リソース消費は抑えやすいが，多数接続・高負荷時はRCと同様に性能低下  \\
		\hline
	\end{tabular}
\end{table}

このRDMA通信はキュー操作としてユーザランドから扱うこととなる．

実装上は，送信キュー（SQ）と受信キュー（RQ）を対にしたQueue Pair（QP）に対してSEND，RECV，WRITEなどの種別のWork Request（WR）を投稿し，
NICがこれをWork Queue Element（WQE）として処理する．
完了通知はCompletion Queue（CQ）に集約され，アプリケーションはCQをポーリングして通信進行を把握する．
この関係を図\ref{fig:rdma-sq-rq}に示す．

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/RDMA-SQ-and-RQ.drawio.pdf}
	\caption{RDMAにおけるSQ，RQ，QPの関係}
	\label{fig:rdma-sq-rq}
\end{figure}

通信開始時，ホストはSQ上にWQEを配置した後，doorbellレジスタへのMMIO書き込みでNICへ新規WQEの存在を通知する．
このdoorbell発行は固定コストを持つため，複数リクエストをまとめて投稿するdoorbell batchingや，ペイロードのinline化によってPCIeトランザクションを削減する設計が有効である．
また，WQEを毎回完了通知付きで発行するとCQ処理負荷が増えるため，実運用では完了時にCQEを発生させないunsignaledを組み合わせて制御するのが一般的である\cite{kalia2016DesignGuidelinesHigh}．
同様に，受信側でもCQポーリング1回ごとに，CQヘッド確認，完了エントリ（CQE）の取り出し，ローカルキュー更新などの固定コストが発生する．
特に，単一コアでは小メッセージ通信ではNIC処理性能よりもCPUおよびNICとのメモリ同期コストが律速となることから，
可能な限りPCIeトランザクションとCPUでのキャッシュミスを抑えるため，
バッチ化して多数のリクエストを同時に扱うべきである．




\section{設計}
\label{sec:design}

\subsection{アーキテクチャ}

提案アーキテクチャでは，ノード外通信の終端を通信デーモンに集約し，アプリケーションプロセスは共有メモリ経由でデーモンへ要求を委譲する．
ここで通信デーモンとサービス処理スレッドは同一プロセス内で動作する．

各アプリケーションプロセスはデーモンと共有する送信バッファを持つ．
この送信バッファは要求専用ではなく，クライアントによるリクエスト書き込みとデーモンによるレスポンス書き込みが同居する双方向領域である．
したがって，この領域は単純な一方向キューではなく，呼び出し単位で確保されたスロットを介して要求と応答を対応付ける．

リモート呼び出し時，クライアントは送信バッファ内の割当済み領域へ要求を直接書き込む．
デーモンは要求内容そのものを解釈してディスパッチするのではなく，どこまで要求が書き込まれたかという進行位置を管理し，
未送信領域に対応するWQEを構築してdoorbellを発行する．
すなわち，デーモンの役割は主に転送境界管理とRDMA送信発行である．
受信した応答はデーモンが対応スロットへ書き戻し，クライアントは自スロットの状態遷移を観測して完了を判定する．

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/in-node-architecture.drawio.pdf}
	\caption{ノード内アーキテクチャとReq/Response書き込みの流れ}
	\label{fig:in-node-architecture}
\end{figure}

一方，ローカルノードで完結する要求はこのリモート呼び出し経路を通さない．
ローカル専用のIPCキューを別途用意し，ノード内処理はそのIPCキューで処理することで，リモート経路の送信バッファを汚染しない設計とする．
このローカルIPCキューはone-sidedアルゴリズムで実装する．
すなわち，要求データ領域はクライアントのみが書き込み，デーモンは読み取り専用とする一方，
完了状態領域はデーモンのみが書き込み，クライアントは読み取り専用とする．
各方向の進行位置（producer/consumer）も単一書き手となるよう役割を固定し，
可視化は「payload/header書き込み後に状態フラグを更新する」順序で行う．
これにより，ノード内通信でもcompare-and-swap中心の多者競合を避けつつ，
ポーリングのみで要求到着と完了を判定できる．

この構成により，ノード間接続，送受信バッファ，フロー制御状態をデーモン側へ集約できるため，
ノード内プロセス数が増えてもノード外通信状態量の増加を抑えられる．
結果として，Fat-node環境で問題となる接続状態メモリと通信制御オーバーヘッドの増大を緩和する．

\subsubsection{QP状態メモリの理論見積り}

提案方式の効果を定量化するため，接続管理に起因するメモリ使用量を理論的に見積もる．
ここで，ノード数を$N$，ノード当たりクライアントプロセス数を$P$，
ノード外RDMA送信を担当するデーモン側実体数を$D$とする．
また，1接続当たりのQP関連状態メモリを$m_{\mathrm{qp}}$ [Byte]，
UCX worker/endpoint等の管理状態を接続当たり$m_{\mathrm{ctl}}$ [Byte]とおく．

ノード当たりの総通信メモリは概念的に，
\begin{equation}
M_{\mathrm{total}} = M_{\mathrm{common}} + M_{\mathrm{conn}} + M_{\mathrm{buf}}
\end{equation}
と分解できる．
ここで$M_{\mathrm{common}}$はノード共通の固定コスト，$M_{\mathrm{conn}}$は接続管理コスト，
$M_{\mathrm{buf}}$はクライアント固有のリクエスト/レスポンス用バッファである．
提案方式とUCXの比較では，$M_{\mathrm{buf}}$は双方で必要となるため差分は主に$M_{\mathrm{conn}}$で生じる．

UCXベースラインではクライアントが個別に通信コンテキストを持つため，接続管理コストは
\begin{equation}
M_{\mathrm{conn}}^{\mathrm{ucx}} = \alpha P (N-1)(m_{\mathrm{qp}} + m_{\mathrm{ctl}})
\end{equation}
で表せる．
ここで$\alpha$は接続粒度を表し，1プロセスがリモート各ノードに1接続を持つ場合は$\alpha=1$，
プロセス間全接続に近い構成では$\alpha=P$となる．

提案方式ではノード外接続をデーモンに集約するため，接続管理コストは
\begin{equation}
M_{\mathrm{conn}}^{\mathrm{prop}} = D (N-1)(m_{\mathrm{qp}} + m_{\mathrm{ctl}})
\end{equation}
となる．
したがって改善率は
\begin{equation}
\frac{M_{\mathrm{conn}}^{\mathrm{ucx}}}{M_{\mathrm{conn}}^{\mathrm{prop}}}=\frac{\alpha P}{D}
\end{equation}
で与えられ，クライアント数$P$の増加に対して，提案方式は$D$で頭打ちの増加に抑えられる．

例えば$N=256$，$P=64$，$D=8$，$m_{\mathrm{qp}}=16$KiBとすると，
$\alpha=1$でもQP関連状態だけで$M_{\mathrm{conn}}^{\mathrm{ucx}}\approx255$MiB，$M_{\mathrm{conn}}^{\mathrm{prop}}\approx32$MiBとなり，
約8倍の削減となる．
さらに$\alpha=P$に近い構成では削減率は約512倍となる．
この削減は，単なるメモリ容量削減だけでなく，NICが保持・参照する接続状態の総量を減らし，
接続状態キャッシュ競合の緩和につながる点に意義がある．

\subsection{RDMA通信プロトコル}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\textwidth]{handmade-figures/LocustaRPC.pdf}
	\caption{LocustaRPCの基本通信フロー}
	\label{fig:LocustaRPC-overview}
\end{figure*}

\subsubsection{転送と通知}

本方式の基本方針は，クライアントが共有メモリ上の送信バッファへ要求を直接書き込み，
デーモンはその書き込み済み範囲をRDMA WRITEで転送し，WRITE with Immediate（WRITE+IMM）で受信側へ到着通知することである．
ここでデーモンは要求ペイロードを解釈しない．デーモンが扱うのは，送信バッファ上のどこまでが送信可能かという境界情報と，
対応するWQE列のみである．

クライアントが共有メモリ上の送信バッファへ直接メッセージを用意することで，ノード内の余分なコピーと同期点を削減し，
デーモンはWQE生成とdoorbell発行に専念できる．

図\ref{fig:in-node-architecture}の送信バッファは，要求と応答が同居する双方向リング領域である．
各要求はスロット単位で管理され，デーモンは複数スロットをバッチ化して1回のRDMA WRITE+IMMで転送する．
受信側は集約された受信CQをポーリングし，1つのCQEをトリガとしてバッチ内の複数メッセージを処理する．
これにより，通知処理はメッセージ単位ではなくバッチ単位へ償却される．

送信バッファは，slot header（call\_id，generation，状態，要求長/応答長）とpayloadからなるスロット列として管理する．
この論理フォーマットと可視化手順は図\ref{fig:message-write-protocol}で合わせて示す．

\subsubsection{フロー制御}

本方式では，送信バッファの進行位置を管理することで受信バッファ利用量も同期的に制御されるため，
送信側の管理だけで受信側バッファが自然にあふれない構成になる．
すなわち，送信可能範囲は対向の受信済み位置により制約され，この範囲を超える書き込みは行わない．
通常時のconsumed位置共有は，要求・応答バッチに付与するflow metadataのpiggybackで行い，
相互のReq/Res送信に合わせてリング使用状況を通知する．

ただし，この制御だけではデッドロックが起こりうる．
典型例は，responseサイズがrequestサイズを上回る条件で，双方がリング上限まで要求を詰めた結果，
応答書き戻しの空きが不足し，相互に応答送信待ちとなる場合である．
この停止を避けるため，本方式では，ローカル送信リングに新規書き込みできなくなった時点で，
対向のconsumed位置をRDMA READで直接取得する．
これにより，piggyback更新が遅れている場合でも最新の空き情報を能動的に回収できる．
さらに受信側は，イベントループが進行する限りResponseが一時的に滞留してもReq処理を止めない．
このため対向のconsumedは前進し続け，最終的に相互待ちが解消されてデッドロックを回避できる．

\subsubsection{送信バッファ書き込み調停}

複数プロセスが同一送信バッファへ同時に書き込むため，書き込み領域の調停と公開順序の保証が必要になる．
本方式では「領域確保」と「公開」を分離した2段階プロトコルを採る．
全体の流れは図\ref{fig:message-write-protocol}に示す．

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{handmade-figures/message-write.drawio.pdf}
	\caption{複数クライアントによるメッセージ協調書き込み}
	\label{fig:message-write-protocol}
\end{figure}

第1段階では，クライアントが原子的なfetch\_addでproducer位置を前進させ，書き込み区間を確保する．
この時点では他プロセスから当該スロットは不可視である．
第2段階で，クライアントはslot header（call\_id，generation，要求長/応答長）とpayloadを書き込み，
最後にstateをReqReadyへ更新して可視化する．
この更新順序により，デーモンはReqReadyを観測した時点で当該スロットの内容を一貫して参照できる．
デーモンは前回処理したconsumer位置から順にslot headerのstateを確認し，ReqReadyの連続区間を送信範囲としてWQEを生成する．

このとき重要なのは，デーモンがPayload内容を見て要求を抽出しない点である．
デーモンはSlot Headerの境界情報と状態遷移のみを用いて送信可能区間を決定する．
結果として，調停対象は「どのスロットが公開済みか」という管理情報に限定され，競合点を最小化できる．

ローカル完結要求は専用IPCキューへ投入されるため，この送信バッファ調停はリモート送信対象の要求にのみ適用される．
すなわち，ノード内delegationの競合制御とノード間RDMA送信の競合制御を分離し，それぞれ最小限の同期機構で実装する．

リング折返し時はgeneration counterを更新して同一オフセット再利用を識別し，
応答書き戻し時にもcall\_idと世代を照合することで，古い要求との取り違えを防ぐ．

\subsubsection{Adaptive Batch Hold制御}

本設計では，デーモンがLocustaRPCから受信した応答をクライアントへ返す段階に
Adaptive Batch Hold制御を導入する．
この制御は，応答を短時間保持してまとめて返すための保持時間$T_{\mathrm{hold}}$を動的に調整する．
Daemon-Daemon通信のようなバッチングで性能が向上するサービス間の通信では，バッチングについて双安定性現象が発生しうる．
すなわち，1ループあたりの到着件数が十分あるときは大バッチ状態に入りやすく，
到着がばらけると小バッチ状態に入りやすい．

背景には，イベントループのコストが「固定コスト」と「動的コスト」に分かれるという性質がある．
固定コストはCQポーリング，IPC到達点チェック，送信可能範囲チェックなど，
1ループごとに必ず発生する処理である．
一方，動的コストはループ内で実際に処理する要求・応答件数に応じて増減する．
ノード数が増えるとリクエスト到着が分散しやすく，1ループあたりの動的コストは小さくなるため，
ループ自体は短く高頻度で回る．しかしその結果，固定コストの支払い回数が増え，
固定コストの償却が崩れて全体スループットが低下するジレンマが生じる．
これを本稿ではバッチ崩壊と呼ぶ．
逆に，到着がある程度まとまる条件では動的コストは増えるが，
ループ時間が長くなることでさらにキューが溜まりやすくなり，大バッチ状態が維持される．

Adaptive Batch Holdは，この小バッチ側への収束を抑えるために，
クライアントへの応答返却を短時間だけ意図的にstallし，返却バッチを再形成する．
要求側ではリモート経路の変動により到着のばらつきが残るため，
制御対象は応答返却段に置く．
実装では，各ループで観測した応答到着件数とループ実行時間から到着率を推定し，
「目標バッチ（本稿では概ねノード数相当）が揃うまで待つ時間」を$T_{\mathrm{hold}}$として更新する．
ただし観測値は離散的で変動が大きいため，推定値には軽量な平滑化手法として加重移動平均（EWMA）を用いて平滑化し，
保持時間の過剰な振動を抑える．

結果として，到着がばらける条件では保持時間をやや長くしてバッチ形成を促進し，
到着が集中する条件では保持時間を短くして待ち時間を抑える適応制御となる．
本制御が必要となる現象の説明と有効性の評価は，実験結果のAdaptive Batch Hold評価（\ref{sec:eval-adaptive-batch-hold}）で述べる．

\section{関連研究}
\label{sec:related-work}

\subsection{FaRM，eRPC，ScaleRPC}

ソフトウェアによるRDMA RPCのスケーラビリティ改善は，特に多数クライアントが少数サーバに接続するワークロードでの「接続状態の扱い」「受信処理の検出コスト」「メモリ階層競合の抑制」という3つの観点で進展してきた．
FaRMはone-sided RDMA read/writeを中核に据え，RDMA writeベースのメッセージングをリングバッファとポーリングで実装した．
同研究は高い性能を示す一方で，チャネル数増加に伴うポーリング負荷と，接続増加に伴うNIC上のQueue Pair状態キャッシュ不足を報告しており，
対策として接続粒度をthread-to-threadからthread-to-nodeへ粗粒化し，NUMAを意識したQP共有を導入して状態量を削減している\cite{dragojevicFaRMFastRemote2014}．
すなわちFaRMのアプローチは，RCベースの高速性を維持しつつ，接続の束ね方をソフトウェアで最適化する方向である．
これに対し本研究は，スレッド単位ではなくノード内の複数プロセス単位で通信を集約し，具体的な共有手法も提案した．

eRPCは，接続状態をNICに保持するRC系設計そのものをスケーラビリティ制約と捉え，packet I/O（UDP/UD相当）を用いてNIC管理状態を削減する設計を採る．
論文中では，RDMA接続数が増えるとNIC SRAM上の接続状態キャッシュ制約により性能が低下することを示し，eRPCでは接続状態をCPU側で管理することで大規模接続時の性能維持を図っている．
加えて，event loopによるdispatch処理，session creditによるエンドツーエンドフロー制御，zero-copy送受信を組み合わせることで，
汎用RPCとしての機能性と高スループットを両立している．
UD系アプローチで性能が伸びる主要因は，(1)NIC側の接続状態保持を削減できること，(2)受信検出をCQベースで定数時間化しやすいこと，(3)接続切替に伴うNICキャッシュミスを避けやすいことである\cite{kalia2019DatacenterRPCsCan}．
一方で，信頼性制御や再送制御，および大きなデータ転送時のバッファ管理をソフトウェア側で担う必要があり，ワークロードによっては設計複雑性が増す．
ad-hoc filesystemにおいてはバッファ転送にRCないしDCが必要であり，完全にUDでの実装は困難である．

ScaleRPCは，RCを維持したままスケーラビリティを高めるため，接続を時間分割で扱うconnection groupingと，
物理メッセージプールを論理的に多重化するvirtualized mappingを提案している\cite{chenScalableRDMARPC2019}．
前者は同時にアクティブ化する接続群を制限してNICキャッシュのスラッシングを抑え，後者はメッセージバッファ共有によってCPU最終段キャッシュ競合とメモリ使用量を削減する．
さらに，優先度ベースのスケジューリングとwarmup機構によりグループ切替オーバーヘッドを隠蔽し，RCの利点を保ちながら大規模接続への適応を図っている．
高いスケーラビリティを実現しているが，多数のノードとの接続を行う場合，プロセスの数だけのクライアントが複数のQPを持つので
ノードレベルでのQP slashingが発生する問題がある\cite{chenScalableRDMARPC2019}．

これら3研究は，いずれもソフトウェアでの工夫によりRDMA RPCのスケーラビリティを改善しているが，設計思想は異なる．
FaRMとScaleRPCはRCを前提に接続状態管理の効率化を狙うのに対し，eRPCはNIC接続状態依存を減らす方向で拡張性を確保する．
また，これらの多くはデータセンター内KVS/DB系ワークロードを主対象としており，要求メッセージが小さく制御可能である前提が強い．
HPC向けad-hocファイルシステムでは，計算ノード内でクライアントとストレージデーモンが同居し，大量同時アクセスと大きなデータ転送が混在するため，
サーバ側だけでなくクライアント側を含む通信資源管理が重要になる．
FaRMの最適化単位はスレッド中心であり，eRPCは汎用性を重視する代わりにone-sided操作前提のデータ転送管理を直接提供しないため，
HPCストレージ向け実装では追加の設計判断が必要となる．
本研究はad-hocファイルシステムを想定し，RCの機能を利用しつつノード内集約で接続状態とメモリ消費を削減する点で，RC維持型アプローチをHPCストレージ文脈に適用する位置づけとなる．

\subsection{Flock}

Flockは，RCの信頼性と機能を維持したまま高fan-in/fan-out環境での接続スケーラビリティを高めるため，
接続ハンドル抽象による接続多重化と，部分共有モデルに基づくスレッド間接続共有を提案している\cite{monga2021BirdsFeatherFlock}．
同研究は，従来の「接続共有は性能を劣化させる」という見方に対し，
coalescingベース同期と送受信スケジューリングを組み合わせることで，同期オーバーヘッドを抑えつつ公平な接続利用を達成できることを示した．
この設計は，RCを前提に接続管理コストを実装レベルで下げる点でScaleRPCと同方向の課題意識を持つが，
主にスレッド間での接続共有と負荷制御に焦点がある．
これに対し本研究は，ノード内の複数プロセスからノード外通信終端をデーモンへ集約することで，
プロセス数増加時の状態量をアーキテクチャレベルで抑制する点に特徴がある．

\subsection{mRPC}

mRPCは，従来の「アプリ内RPCライブラリ＋サイドカー」構成における重複した(アン)マーシャリングを問題設定とし，
RPCを管理対象のシステムサービスとして提供するアーキテクチャを提案している\cite{chen2023RemoteProcedureCall}．
同研究の中核は，マーシャリングとポリシー適用を同一の特権サービス側に集約し，アプリケーションは共有メモリキュー経由でRPC要求を受け渡す点にある．
これにより，サイドカー方式で生じるデータコピーと再マーシャリングを削減しつつ，アクセス制御・レート制御・可観測性機能を実行時に挿入できる．

設計面では，mRPCサービス内部をポリシー，トランスポート，フロントエンド等のエンジンに分解し，
エンジンの動的バインディングとライブアップグレードを可能にしている．
この機構により，アプリケーションを再コンパイル・再起動せずに，ポリシー，トランスポート，マーシャリング実装を更新可能とする点が特徴である．
評価では，gRPC+EnvoyやeRPC+proxyに対して，特にサイドカー経由時の冗長処理を削減することで，レイテンシとスループットの改善を報告している．

本研究との類似点は，ノード内でRPC処理をサービス側に寄せ，アプリケーション側との通信に共有メモリを活用することで，
ネットワーク経路へ投入する前の処理を集約する点にある．
一方で目的は大きく異なる．mRPCの主眼はクラウド/マイクロサービス環境における運用性（ポリシー柔軟性，可観測性，無停止更新）の向上であり，
接続状態量やRC通信資源の削減そのものは第一目的ではない．
これに対し本研究は，HPC向けad-hocファイルシステムにおいて，ノード内の複数プロセスから生じる通信を集約し，
接続状態とメモリ消費を抑えて大規模実行時の通信スケーラビリティを確保することを主目的とするが，
情報をDaemonが集約することでクライアントに必要な情報を減らすことはSidecarパターンとの類似点もある．

\section{実験}
\label{sec:evaluation}

\subsection{実験環境}

実験には，筑波大学計算科学研究センターのPegasusスーパーコンピュータを用いた．
評価にはPegasusの計算ノードを用い，各ノードはIntel Xeon Platinum 8468（48コア）を搭載する．
ネットワークはInfiniBandで接続し，NICはMellanox ConnectX-7を用いた．
評価時はNUMAノードが1つの構成を用い，コア0および1はIRQ処理に割り当て，ベンチマーク実行には使用しない．

\subsection{実験手法}

提案手法の効果は，通信ライブラリ単体ではなく，ベンチマーク用Key-Value Store実装を用いたend-to-end測定で評価する．
ベンチマーク用Key-Value Store実装は，keyをrank idとユーザキーの組で管理する単純なKVSであり，アプリケーションロジックの複雑さを抑えて通信層の影響を観測することを目的とする．
クライアント要求は設計節で述べた共有メモリ経由でデーモンに委譲され，デーモンがRDMA通信を実行する．

アクセスパターンはYCSB-likeなread/write混合負荷として定義する．
まず初期化フェーズで測定対象キー集合をPUTして事前投入し，測定フェーズのGETはこの事前投入済みキー集合のみを対象とする．
したがってGETは既知キーに対する読み出しとなり，存在しないキー探索の影響を排除して通信層の差を比較する．
PUTは同一キー集合に対する更新として扱い，測定中にキー集合サイズが増大しないようにする．

read/write比率は，実験で使用した50/50（read ratio=0.5）の1条件を用いる．
この条件でノード内クライアントプロセス数を変化させ，提案アーキテクチャの主眼である
ノード内プロセス増加時の通信資源効率と性能スケーラビリティを検証する．
評価指標はスループット（IOPS）を用いる．

また，提案方式の有効性を明確化するため，比較対象にはUCXを用いる．
UCXベースラインでは，各クライアントプロセスが個別にUCX通信コンテキスト（worker/endpoint）を保持し，
要求発行時に各プロセスから直接通信を行う構成とする．
UCXは通信相手の配置に応じてトランスポートを自動選択するため，
同一ノード内通信は共有メモリ系トランスポートが使われ，リモートノードとの通信はRDMAトランスポートが選択される．
本評価ではこのUCXの標準的な通信モデルをそのまま用い，ノード内集約デーモンを介した提案方式と比較する．
これにより，クライアント数増加に対するエンドツーエンド性能への影響を同一条件で比較する．
さらに，分析用ベースラインとしてdirect connectionも用いる．
direct connectionは，提案方式と同じRPC実装を使いながら，ノード内集約を行わず，
各クライアントがリモートノードへ直接通信する構成にしたものである．
すなわち，提案方式との差分を通信経路（集約の有無）に限定し，UCXとの差分をRPC実装差に限定して比較できるようにする．
この比較により，性能差だけでなく，接続状態管理とメモリ使用の観点での効果を評価する．

\subsection{実験結果}

\subsubsection{基本性能}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/kvs_benchmark_throughput.pdf}
	\caption{ベンチマーク用Key-Value Store実装におけるノード数に対する総スループット（MIOPS）}
	\label{fig:kvs-benchmark-throughput}
\end{figure}

図\ref{fig:kvs-benchmark-throughput}に，ベンチマーク用Key-Value Store実装の総スループットを示す．
ノード数2および4では，提案方式（proxied connection）がそれぞれ14.9 MIOPS，21.1 MIOPSとなり，
direct connection（2.83，7.37 MIOPS）およびUCX（3.38，6.73 MIOPS）を上回った．

一方，ノード数8では提案方式が11.2 MIOPSまで低下し，16ノードで19.6 MIOPSへ回復する非単調な挙動を示した．
この8ノードでの低下は，リモート通信経路におけるbatching崩壊が生じ，CQEバッチが十分に形成されない小バッチ状態へ遷移したためと解釈できる．
具体的には，バッチが小さくなると1回のポーリングループ時間が短くなり，次ループまでに新規CQEが十分に蓄積しなくなる．
すると小バッチ処理がさらに続き，CQポーリング，ローカルキュー更新，送信発行準備といったローカル通信側の固定コストが支配的となる．
特に本実装では，IPCでのメッセージ到達点チェックと送信バッファの送信可能範囲チェックが定常的に発生し，
これらはクライアントとデーモン間で同一キャッシュラインを更新・参照するためcache line ping-pongを生みやすい．
その結果，メモリアクセス遅延が増加し，バッチングが崩れた状態ではこの固定コストが相対的に支配的になる．
この正のフィードバックにより，スループットが低い状態へ収束する．
ノード数32ではUCXが66.5 MIOPS，direct connectionが62.7 MIOPS，提案方式が39.2 MIOPSであった．

\subsubsection{Adaptive Batch Holdの評価}
\label{sec:eval-adaptive-batch-hold}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/kvs_benchmark_batch_hold.pdf}
	\caption{Adaptive Batch Hold有無の比較（ベンチマーク用Key-Value Store実装の総スループット）}
	\label{fig:kvs-benchmark-batch-hold}
\end{figure}

\begin{table}[t]
	\centering
	\caption{Adaptive Batch Hold有無の比較（MIOPS）}
	\label{tab:kvs-benchmark-batch-hold}
	\begin{tabular}{c|c|c|c}
		\hline
		ノード数 & no batch hold & batch hold（10$\mu$s） & 改善率 \\
		\hline
		2  & 13.97 & 14.86 & 1.06$\times$ \\
		4  & 22.11 & 21.10 & 0.95$\times$ \\
		8  & 8.26  & 11.18 & 1.35$\times$ \\
		16 & 14.86 & 19.63 & 1.32$\times$ \\
		32 & 27.25 & 39.24 & 1.44$\times$ \\
		\hline
	\end{tabular}
\end{table}

図\ref{fig:kvs-benchmark-batch-hold}および表\ref{tab:kvs-benchmark-batch-hold}に，提案方式におけるAdaptive Batch Holdの有無を比較した結果を示す．
no batch holdに対してbatch holdは，2ノードで13.97から14.86 MIOPS（1.06倍），
8ノードで8.26から11.18 MIOPS（1.35倍），16ノードで14.86から19.63 MIOPS（1.32倍），
32ノードで27.25から39.24 MIOPS（1.44倍）へ改善した．
4ノードでは22.11から21.10 MIOPSへわずかに低下したが，
全体として中大規模条件では保持時間制御により霊天使増大を補いバッチサイズを維持することでスループットが向上することが確認出来た．

以上より，提案方式はローカル要求比率が高い条件で高い有効性を示す一方，
中規模条件ではbatching維持機構の設計が性能を左右することが分かる．
したがって今後の最適化では，デーモン側並列度の拡張に加えて，
小バッチ状態への遷移を防ぐバッチング制御を導入する必要がある．

\section{まとめ}
\label{sec:conclusion}

本稿では，Fat-node環境を対象として，ノード内共有メモリによる要求委譲と
デーモンへのノード外通信集約を組み合わせたRPC基盤を設計・実装した．
設計面では，接続管理コストの理論モデルを示し，クライアントごとに通信コンテキストを持つ構成に比べて，
提案方式がノード当たり通信状態量を大きく削減できることを示した．
この結果は，通信リソース消費を抑えたいHPC向けミドルウェアに対して，提案方式が有効であることを示す．

また，関連研究であるFaRM，eRPC，ScaleRPC，Flock等の既存RDMA RPCが主に接続管理やトランスポート最適化を議論してきたのに対し，
本研究はノード内プロセス群の通信をデーモンへ集約することでスケーラビリティを改善するという，
異なる設計軸を提示した．
特に，POSIXを前提とするad-hocファイルシステムのようにアクセス先の事前制約が難しい条件で，
通信状態の増加をアーキテクチャ段階で抑制する点が本研究の主要な貢献である．

一方で，評価からは課題も明確になった．
低Queue Depth条件では，イベントループ1回あたりの到着リクエスト数が不足すると
batching崩壊が生じ，ローカル固定コストが支配的になることに加えて，
doorbell batchingが効かない状態へ遷移して性能が低下する．
Adaptive Batch Holdによりこの崩壊を一定程度緩和できることは確認できたが，
現時点では十分なスケーラビリティを達成できていない．
また，Pegasus環境では実験可能なノード数に制約があり，
提案方式の大規模条件での挙動を十分に検証するには至っていない．

今後の課題は次の2点である．
\begin{enumerate}
		\item DCを用いてSQおよび送受信キューの管理を一本化し，提案方式を通常のDC通信にIPCキュー操作コストが加わるモデルへ近づけ，性能低下をIPC由来の固定オーバーヘッドへ限定すること
	\item 簡易なKVSベンチマークに留まらず，実際のad-hocファイルシステム上で評価し，より大規模なノード数におけるスケーラビリティを検証すること
\end{enumerate}
これらを通じて，POSIX志向ワークロードにおいても安定して高性能を維持できる通信基盤を目指す．

\begin{acknowledgment}
本研究の一部は，JSPS科研費JP22H00509，
NEDO（国立研究開発法人新エネルギー・産業技術総合開発機構）の委託事業「ポスト5G情報通信システム基盤強化研究開発事業」（JPNP20017），
JST CREST JPMJCR24R4，
筑波大学計算科学研究センター学際共同利用プログラム，および富士通との特別共同研究の結果得られたものです．
\end{acknowledgment}

\bibliographystyle{ipsjsort}
\bibliography{filesystem}

\end{document}

%%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,eabstractは任意)
%% [noauthor]
%%

%\documentclass[submit,techrep]{ipsj}
\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage[dvips]{graphicx}
\usepackage{latexsym}
\usepackage{url}

% ipsjクラスの\doiがDOI中のアンダースコアを処理できない問題を回避
\makeatletter
\renewcommand{\doi}[1]{\url{https://doi.org/#1}}
\makeatother

\begin{document}

\title{LocustaRPC: 次世代リーダーシップマシンのための\\スケーラブルなRPC基盤}

\etitle{LocustaRPC: A Scalable RPC Framework\\for Next-Generation Leadership Machines}

\affiliate{UT}{筑波大学\\
University of Tsukuba}

\author{中野 将生}{Masaki Nakano}{UT}
\author{前田 宗則}{Munenori Maeda}{UT}
\author{建部 修見}{Osamu Tatebe}{UT}

\begin{abstract}
次世代リーダーシップマシンでは数十万ノード規模のシステムが想定されており，
従来のRPC基盤ではスケーラビリティが課題となる．
本稿では，大規模システム向けのスケーラブルなRPC基盤であるLocustaRPCを提案する．
LocustaRPCは，RDMAを活用した低遅延通信と，
スケーラブルなコネクション管理機構を備える．
評価の結果，既存手法と比較して高いスケーラビリティを達成することを確認した．
\end{abstract}

\maketitle

\section{はじめに}

現代のHPCシステムでは、富岳NEXT、Sirius、El Capitanに代表されるように、
ノード内並列性を大幅に向上させたFat node設計が主流となりつつある。
この傾向により、ノード数は数千規模に抑えられる一方で、ノードあたりのプロセス数は数百から数千に及び、
システム全体のプロセス数は数百万のオーダーに達する。

こうした環境において、HPCミドルウェア、特にad-hocファイルシステムやキーバリューストアの
RPC基盤にはスケーラビリティの課題が生じる。
従来のRPC設計では個々のクライアントがNICを直接利用するため、
プロセス数の増大に伴い接続数が爆発的に増加し、NICリソースの枯渇やメモリ消費の増大を招く。

既存のRDMA RPCシステム\cite{kaliaDatacenterRPCsCan2019,kaliaFaSSTFastScalable2016,chenScalableRDMARPC2019,monga2021BirdsFeatherFlock}は
主にネットワーク層の最適化やNICキャッシュの効率化に注力してきた。
しかし、現代のRDMA NICは十分に高速であり、
少数コアでNICを操作する設計においては、真のボトルネックはCPUがどれだけ効率的にメモリ操作を行えるかにある。

そこで本研究では、ノード内IPC、プロセス内デリゲーション、ノード間RDMA通信の全ての通信層を
メモリアクセス最適化問題として統一的に扱うRPC基盤、LocustaRPCを提案する。
LocustaRPCは、ノードごとに配置されたサービスデーモンがリクエストを集約し、
接続数を削減しつつQueue depthを活用する設計により、Fat node環境でのスケーラビリティを実現する。

本稿の貢献は以下の通りである。
\begin{enumerate}
    \item メモリアクセス最適化を軸とした、デーモン中継型RPCアーキテクチャの設計
    \item ibverbsをバイパスした直接デバイスアクセスによるCQポーリング効率化と、
          memory pollingとの比較における従来評価の再検討
    \item キャッシュ制御（CLDEMOTE、HITM回避）を意識したノード内通信の設計と最適化
    \item Connect-X7上でのEnd-to-End性能評価
\end{enumerate}

\section{関連研究}

\subsection{WRITE系RDMA RPCとデータ配置・到着通知}

RDMA WRITEを用いたRPCは、送信側がone-sided WRITEで受信側メモリへ要求・応答を直接配置し、
受信側CPUが到着を検知して処理する系統であり、
データ配置と到着通知の設計が性能を左右する。

メモリポーリング型の代表であるFaRM\cite{dragojevicFaRMFastRemote2014}は、
受信側のcircular bufferにRDMA WRITEでメッセージを書き込み、
受信側がHead位置をポーリングして非ゼロ値により到着を検知する。
しかしチャネル数増加に伴いポーリング対象が線形に増加する問題があり、
FaRM自身が大規模化時にはWRITE with Immediate Data（WRITE+IMM）とShared Receive Queue（SRQ）による
ポーリングコストの一定化が望ましいことを示唆している。

HERD\cite{kalia2014UsingRDMAEfficiently}はRDMA WRITEで要求を書き込み、
サーバCPUがメモリをポーリングして検知するが、
outbound WRITEが接続数でスケールしない問題を回避するため応答はUD SENDで返すハイブリッド設計を採る。

研究史的にはCQポーリングはibverbs経由のspin lockやコピーのオーバーヘッドにより高コストとして敬遠されてきた。
Fentら\cite{fent2020LowLatencyCommunicationFast}も
WRITE+IMMとメモリポーリングを比較検討し、
CQポーリングが無条件に有利でないことを可視化している。

本研究は、ibverbsをバイパスしたCQハードウェアバッファへの直接アクセスと
複数QPのCQイベント集約により、CQポーリングの再評価を行う。
これはFaRMが示唆した「WRITE+IMM+SRQによるポーリングコスト一定化」を具体的に実装・体系化したものと位置づけられる。

\subsection{RDMA RPCのフロー制御とプロトコル設計}

eRPC\cite{kaliaDatacenterRPCsCan2019}は汎用的かつ高速なRPCフレームワークであり、
session creditsによるend-to-endフロー制御でスイッチキューイングを抑制する。
本研究のcredit\_grant/resp\_reservation設計は同じcredit系譜にあるが、
eRPCはcommodity UDP上の設計であり、NIC直接操作によるFat node向け最適化は行っていない。

FaSST\cite{kaliaFaSSTFastScalable2016}はtwo-sided UD datagram RPCを採用し、
各コアが1つのdatagram QPで多対多通信を行うことでQP状態爆発を回避する。
本研究はRC + WRITE+IMMで対極に位置するが、
デーモン中継による接続集約でRCの接続数問題に対処する。

RFP\cite{wuRFRPCRemoteFetching2019}は、
サーバのoutbound RDMA発行コストが高くinbound/outboundに非対称性があること、
server-bypassが複数回の追加RDMAを要しがちであることを分析し、
応答をクライアントがREADで取りに行くパラダイムを提案した。
本研究は応答もWRITEで返す設計だが、
バッチングによるWQE・doorbell節約、piggybackによる追加往復抑制、
予約不変条件による応答詰まり抑制が、outbound非対称性をどの程度相殺できるかが評価上の論点となる。

HatRPC\cite{li2021HatRPCHintacceleratedThrift}は、
payloadサイズ・並行度・性能目標に基づき、
Direct-WriteIMM、RFP、Send/Recv等の複数RDMAプロトコルを切り替える設計空間を提示し、
WRITE+IMMが常に最適ではないことを示した。
本研究はWRITE+IMM中心に設計を収束させているため、
勝つ条件領域（小〜中メッセージ、低レイテンシ重視、lossless前提）を明確に評価する。

Weiら\cite{wei2018DeconstructingRDMAenabledDistributed}は
分散トランザクション文脈で、one-sided/two-sidedのどちらか一方が全フェーズに勝つわけではないことを
phase-by-phaseで示し、ハイブリッドが最適と結論した。

\subsection{接続スケーラビリティ}

RC QPの状態爆発（RNIC SRAM不足によるPCIe read増加と性能低下）は、
2020年代のRDMA研究で最も重要な争点の一つである\cite{kong2023UnderstandingRDMAMicroarchitecture}。

ScaleRPC\cite{chenScalableRDMARPC2019}はconnection groupingで同時アクティブ接続数を抑え
NICキャッシュスラッシングを緩和し、virtualized mappingでメッセージプール共有によりCPUキャッシュ競合を抑制する。
ただし、グルーピングに起因するレイテンシ分布の二峰性というトレードオフがある。

Flock\cite{monga2021BirdsFeatherFlock}はconnection handle abstractionで
スレッドとQPを多対多に多重化し、coalescingベースの同期とcredit renewalでスケーラビリティを改善した。

RDMAX\cite{ma2025RdmaxScalableRDMA}はRC QP multiplexingで
1つのRC QPに複数クライアントを束ね、
in-network request dispatchingで衝突のない書込み位置決定をネットワーク側へオフロードする。

SRC\cite{zhao2025SRCScalableReliable}はQPと``接続''の結合自体を問題視し、
QPとconnectionsをdecoupleすることで状態量を桁違いに削減する。
512ノード規模で、RCの146\,MB/nodeに対しSRCは0.19\,MB/nodeを報告している。

SRNIC\cite{wang2023SRNICScalableArchitecture}はRNICのon-chip状態をモデルに基づき分解し、
プロトコル・アーキテクチャ共同設計で10K接続規模を性能劣化なしに達成した。
StaR\cite{wang2021StaRBreakingScalability}は高並行側のRNICをstateless化により軽量化し、
1RMA\cite{singhvi20201RMAReenvisioningRemote}はマルチテナントDC制約を踏まえ
one-sided primitives中心で send/recvはソフトウェアで代替する再設計思想を示した。

本研究のCQ集約とSRQ共有は「受信側資源管理の定数化」としてSRC等の共有化潮流と整合するが、
RC QPを接続ごとに保持する限りQP状態量問題は残る。
本研究ではデーモン中継による接続集約で
QP数そのものを物理的に削減するアーキテクチャ的回答を採用しており、
ソフトウェアQP共有（Flock）やグルーピング（ScaleRPC）とは異なる比較軸を提供する。

\subsection{RDMA設計指針とNICマイクロアーキテクチャ}

Kaliaらは高性能RDMAシステムの設計指針として、doorbell batching、DDIO、NUMA配置の重要性を示した\cite{kalia2016DesignGuidelinesHigh}。
本研究はこれらの知見をノード内通信からノード間通信まで、スタック全体に拡張して適用する。

Kongら\cite{kong2023UnderstandingRDMAMicroarchitecture}はRDMA NICのマイクロアーキテクチャリソース（QPコンテキストキャッシュ等）を定量的に分析し、
QP数増加時の性能低下メカニズムを明らかにした。
本研究では、Connect-X7においてはQP数1000程度まで実用的な性能が維持されることを踏まえ、
アーキテクチャレベルでの接続集約によりQP数を管理可能な範囲に抑える設計を採用する。

\subsection{ノード内通信とデリゲーション}

Calciuら\cite{calciuMessagePassingShared2013}はマルチコア環境における
メッセージパッシングと共有メモリの性能を定量的に比較し、
デリゲーションの有効性と限界を示した。
本研究はこの知見を踏まえ、SPSCベースのデリゲーション機構を採用しつつ、
キャッシュコヒーレンシプロトコルの影響を最小化するメモリアクセス設計を行う。

mRPC\cite{chen2023RemoteProcedureCall}はRPCのmarshallingとポリシーを
システムサービスとして統合し、サイドカー比で大幅な高速化を報告した。
本研究は通信基盤の極限最適化に重心を置くが、
mRPCが示す``RPCのサービス化''の方向性は、
デーモン中継アーキテクチャと相補的な位置づけにある。

\section{設計}

現代のHPCノードは、富岳NEXT、Sirius、El capitanといったようにノード内並列性を向上させつつノード数自体は抑えるFat node設計が増えてきている。
ムーアの法則が半導体プロセスの改善とマイクロアーキテクチャ改良によるクロック、IPCの向上よりも、ノード内並列性向上の寄与が大きくなり変質してきたことがその理由である。
そのため、現代的な設計ではノード数自体はむしろ減少し、ネットワークトポロジとしては高い性能が標榜されている一方、
通信ライブラリにおいては未だ個別のクライアントがNICを直接利用する設計が多く、ここにギャップが存在する。
% RDMAXとかのHWによるスケーラビリティと比較して検討する

そこで、本研究においては、計算ノード上でのミドルウェア展開という枠組みを踏まえ、以下を目的としたミドルウェア設計の提案を行う。

% sfence集約は具体的すぎる

\begin{enumerate}
    \item Daemonへのリクエスト集約
    \item Daemonへのリクエスト移譲による接続数節約と、それによるメモリ使用量・スループットの改善
    \item 通信のメモリアクセスパイプライン最適化と、sfence集約によるIPC向上
\end{enumerate}


以降、これら3つの目標を達成するために、メモリアクセス最適化を軸とした提案手法を述べる。

\section{提案手法}

\subsection{トポロジ設計}

富岳NEXTを例にとると、ノード数としては3400ノード規模であるものの、プロセス数は数百万のオーダーと見積もられている。
ノード間接続3400自体を単純にサポートするのは現実的である一方で、単純にクライアントがサーバ一つに数百万接続というのは現実的ではない。
そこで、分散第一階層ファイルシステムとして、各ノードに配置されたサービスデーモンを経由してクライアントが他ノードのデーモンと通信するアーキテクチャを導入する。
ノード内通信では、L3 store + L3 loadもしくは、キャッシュコヒーレンシプロトコルによるL1-L1転送のおよそ100 ns程度のレイテンシで通信が可能である。
一方で、ノード間通信ではInfiniband latencyでの3 us程度のレイテンシがかかるため、ノード内でのリクエスト集約が理想的である。

本アーキテクチャにおいては、レイテンシ最適化を行いつつも10 MRPS/daemon以上のスループットを出せるクライアント-デーモン通信と、
Queue depthに対してスケーラブルなDaemon-Daemon間のRPC層を用いて、デーモンがリクエストを集約することで、
Fat nodeによる大規模環境においても安定的かつ柔軟なミドルウェア通信が行えるアーキテクチャを提案する。

また、ノード間接続を最小化するため、ノード間での接続をデーモンでシャーディングする。クライアントはローカルの中継デーモンをクライアントがわで計算してそのローカルデーモンへ直接リクエストを送り、
受け取ったデーモンが代理で外部のデーモンとの通信を行う。
処理担当部分について、自分のコアとは異なるコアが担当するリクエストを受け取った場合は、
最適化されたSPSCベースの処理移譲機構による、in-procedure callでリクエストを処理させ、返答を元にRPCで返答を行う。

これらは小メッセージ向けの最適化パスだが、ファイルシステムでの活用となるとバッファ転送もまた必要である。
小メッセージではリクエストにおける加工される領域が多いこと、コピーコストが十分に小さいことからZero-copyよりもキャッシュコヒーレンシを重視した設計を行う。
これはコア間転送においてはキャッシュライン単位での読み書きでないとfalse sharingが起きて性能が低下することから、
Store bufferが詰まらない限りにおいて64Bまではコピーでも十分効率的であることが理由である。
バッファ転送については、MB単位でのアクセスがあり得るため、コピーコストが大きくなる。
そこで、コピーを最小化するため、クライアントとサーバの/dev/shmによるメモリ共有領域をデーモン側で直接MemoryRegion登録し、
デーモンがそのMemoryRegionに対するRDMAを行うことで大規模メモリ転送をNIC HWに委任する。
これにより、デーモンを中継させたとしてもクライアントへのバッファ転送へメモリコピーを行う必要がなくなる。

\subsection{RPC設計}

通常POSIXインターフェースでは同期的アクセスとなるので、1 node中のリクエストを集約してもQueue depthはあまり稼げない。
% できれば: 既存のHPCアプリでの命令をカウントしておく
重要であるのは、RQ/SQをリソース上のボトルネックから外してRNR retryを防ぐこと、レイテンシを最小化しつつスループットを向上させることである。
% 関連研究を引く。多分one-sided擁護が多いので困らないはず。ただし、これらはibverbsのオーバーヘッドが原因というのは注意
設計上でノード内リクエストの集約によって接続数を絞るので、N-to-N通信ではなく、1-to-1通信をノードごとに用意し、end-to-endのflow controlによって確実な通信を行う。
3000ノード級に対応するため、CQEにイベントを集約してpollingコストを減らす。
従来の研究ではバッファ上にvalidフラグをおいてそこをpollingする手法が提案されていたが、これはibverbsのAPIがthread safetyを提供するために各種オーバーヘッドが大きかったためであり、
CQEもバッファも同じくNICが書き換えるメモリのPollingに過ぎず、現代のアーキテクチャにおいてはMESIFの最適化により書き換えは重大なボトルネックとはならない。
% 従来研究のサーベイ論文があったのでそれを引く。で、CQEに対するDDIOを分析。あとFast-ForwardでのI/E遷移の話もしよう。
% I/E遷移より実はL3→L1昇格の方が若干だけ速いってのも重要な話
% 出来れば比較をする
むしろ、ポーリング領域を一本に集約し、NIC HWに通知集約をオフロードすることによる性能向上が重要である。

そこで、バッファ管理コストを低減し高速な通信を行うため、WRITE with Immediateを中心に設計する。

call/responseのみをモデルとしてサポートし、送信リング、受信リングでのバッファ間転送としてRPCを実現する。
逆方向の通信でも同様にリングを使うので、それぞれエンドポイントは対向あたり送受信2つの固定長リングバッファを持つ。
受信はWrite with ImmediateのImmediate通知に集約し、Immediate 32 bitをEndpoint IDとする。
また、それぞれのbulk messageの先頭には単調増加するPiggybacked seq, message countをもち、個別のメッセージについては、
len, payloadを持つ。受信した際はImmediateでEndpoint IDを直接特定し、
先頭を読んでPiggybacked seqでフロー管理を更新し、それ以降の受信メッセージをmessage countに達するまで読み込む。

seqは受信して上書き可能になったメッセージの容量であり、バッファはメッセージごとに64B alignされているので64 B blockの数として表現される。
このseqを各バルクメッセージの先頭にpiggybackすることでflow controlを行う。

ただし、seqのみではデッドロックが発生しうる。
seqの通知を受けない限りバッファ解放を知れないが、seqを送るには対向のリングに空きが必要である。
双方がリクエストでリングを埋め尽くすとレスポンスを書き込めず、seqが伝播しなくなる。

\subsubsection{クレジットベースのレスポンス予約}

この問題を解決するため、クレジットベースのレスポンス予約機構を導入する。
リング容量を$C$ブロックとし、各リングの書き込み側が以下の3量を管理する。
\begin{itemize}
  \item $r$: 対向に未消費のリクエストブロック数
  \item $s$: 対向に未消費のレスポンスブロック数
  \item $R$: レスポンス予約総量（発行済みクレジットから応答書き込み済み分を引いた値）
\end{itemize}
書き込み側は全ての書き込みおよびクレジット発行に先立ち、以下の不変条件を検査する。
\begin{equation}
  r + s + R \leq C \label{eq:invariant}
\end{equation}

クレジットはseqと共にバルクメッセージ先頭にpiggybackして発行する。
対向はクレジット残高の範囲内でのみリクエストを送信でき、各リクエストにレスポンス許容量$R_i$を設定する。
各操作に対する不変条件の変化を表\ref{tab:invariant}に示す。

\begin{table}[t]
  \caption{各操作に対する不変条件$r + s + R \leq C$の変化}
  \label{tab:invariant}
  \centering
  \begin{tabular}{l|ccc|c}
    \hline
    操作 & $\Delta r$ & $\Delta s$ & $\Delta R$ & 変化量 \\
    \hline
    リクエスト送信 ($q$ブロック) & $+q$ & & & $+q$ ※ \\
    クレジット発行 ($g$ブロック) & & & $+g$ & $+g$ ※ \\
    対向リクエスト受信 ($R_i$) & & & $\pm 0$ & $0$ \\
    レスポンス書き込み ($s_i \leq R_i$) & & $+s_i$ & $-R_i$ & $s_i - R_i \leq 0$ \\
    Seq受信 (消費量$x$) & $-x$ or & $-x$ & & $-x$ \\
    \hline
  \end{tabular}
  \\※ 事前に不変条件を満たすことを確認
\end{table}

対向リクエストの受信時、$R$の内訳は変化する（未使用クレジットが未応答予約に移行する）が総量は不変である。
レスポンス書き込み時は$s$が$s_i$増加し$R$が$R_i$減少するため、変化量$s_i - R_i \leq 0$で不変条件は保たれる。

この不変条件から、レスポンスの書き込み可能性が導かれる。
式(\ref{eq:invariant})よりリングの空き$C - r - s \geq R$が成立する。
$R$は未応答リクエストの許容量$R_i$を含むため$R \geq R_i \geq s_i$である。
よって空き$\geq R \geq R_i \geq s_i$となり、レスポンスの書き込みは常に成功する。

固定的なリング分割と異なり、クレジット発行量を動的に調整することで、
トラフィックパターンに応じたリング利用率の最適化が可能である。
片方向通信時にはリング全体をリクエストに充当でき、
双方向通信時には適応的にレスポンス予約を確保する。

これらの処理については、送信API単体では実際には送信されず、poll()の呼び出しによって進行されるようにすることでバッチングを行い、
NIC負荷を下げてQueue depthを活用する設計としている。

\subsection{Client-Daemon通信}

Client-Daemon通信はx86\_64のTSOを活用し、SPSCでOut-of-Order返答が可能なリングを用いる。
クライアントごとにLaneを分離し、サーバ側が全てをPollingすることでクライアント調停コストを最小化する。
また、Laneに対して連続的に多重化されたリクエストをDrainすることで性能を稼ぐ。

\subsection{メモリアクセス最適化戦略}

前節までの設計を横断する共通の最適化原理として、本節ではメモリアクセスの観点からの最適化戦略を述べる。

\subsubsection{キャッシュコヒーレンシ最適化}

コア間通信において、キャッシュコヒーレンシプロトコル（MESIFのForward遷移等）によるL1-L1転送は
最も直接的なデータ共有手段である。
しかし、実測ではCLDEMOTE命令によりデータを明示的にL3に降格させ、
他コアがL3から読み出す方がレイテンシが低い場合がある。
これは、Forward遷移に伴うスヌープのオーバーヘッドが、
L3の共有キャッシュとしてのアクセスレイテンシを上回るためと考えられる。

本設計では、SPSC通信においてプロデューサ側でCLDEMOTEを活用し、
コンシューマがL3から効率的に読み出せるようにする。
また、キャッシュライン境界での64Bアラインメントと、Lane分離によるfalse sharingの回避、
SPSC設計によるHITM（Hardware Interrupt To Modify）イベントの削減を行う。

\subsubsection{Store Buffer最適化}

x86\_64のTSO（Total Store Ordering）メモリモデルでは、
ストアはStore bufferを経由して順序保証される。
これを活用し、SPSCリングバッファにおいてlock-freeなプロデューサ-コンシューマ通信を実現する。

さらに、poll()駆動の送信モデルにおいて、複数のリクエストを蓄積してから一括送信することで、
sfence（Store Fence）の発行回数を集約し、Store bufferの圧力を低減する。
リングバッファへの逐次書き込みにおいては、Write combiningが期待できる連続アドレスへの書き込みパターンを維持する。

\subsubsection{ibverbsバイパス}

libibverbsは汎用的なRDMAアクセスを提供するが、
thread safety保証のためのspin lockや、APIレイヤでの余分なメモリコピーが
性能上の大きなオーバーヘッドとなる。
特にCQポーリングにおけるspin lockは、memory pollingがCQポーリングより高速であるという
従来の評価結果に大きく影響していると考えられる。

本実装ではmlx5デバイスのメモリマップドレジスタに直接アクセスし、
WQE（Work Queue Element）の構築とCQE（Completion Queue Entry）の読み取りを
ibverbsを介さずに行う。
これにより、spin lockの排除とコピー削減を実現し、
CQポーリングがmemory pollingと同等以上の性能を発揮できることを示す。

\subsubsection{NIC-CPU協調最適化}

3000ノード級の環境では、ノードあたり数千のQPが存在し得る。
個別のQPごとにポーリングを行うとCPUコストが線形に増加するため、
複数QPのCompletion Eventを単一のCQに集約し、NIC HW側で通知の集約を行わせる。
これにより、ポーリング対象を1箇所に絞りつつ、各QPの受信完了をImmediate値で識別する。

また、NICのDDIO（Data Direct I/O）によりCQEはL3キャッシュに直接書き込まれるため、
ポーリングループではL3からの読み出しのみで完結する。
送信側ではdoorbell batchingにより、複数WQEに対して1回のMMIO書き込みで通知を行い、
PCIeトランザクション数を削減する。

\section{実装}

LocustaRPCはRust言語で実装されている。
主要なコンポーネントは以下のクレートで構成される。

\subsection{システム構成}

mlx5クレートはConnect-X系NICへの直接アクセスを提供し、ibverbsに依存しない。
copyrpcクレートはmlx5上にRPC通信を実装し、
ipcクレートは共有メモリベースのClient-Daemon通信を、
inprocクレートはデーモン内のスレッド間デリゲーションを提供する。

\subsection{直接デバイスアクセス}

mlx5クレートでは、NICのデバイスファイルを直接mmapし、
WQEの構築、doorbellの発行、CQEの読み取りをユーザ空間から直接行う。
これにより、ibverbsのspin lockおよびAPI層でのメモリコピーを完全に排除する。
QPの生成やMemory Region登録等の制御パス操作についてはibverbs経由で行い、
データパスのみを直接アクセスとすることで、実装の複雑性を抑えている。

\subsection{メモリ管理}

IPC通信では、クライアントとデーモンが/dev/shmを介してメモリ領域を共有する。
デーモンはこの共有領域をNICにMemory Regionとして登録することで、
大規模バッファ転送においてクライアント-デーモン間のメモリコピーを不要にする。
リングバッファはキャッシュライン（64B）アラインで確保し、false sharingを防止する。

% TODO: perf c2cによるHITM計測、Store buffer監視の具体的手法を記述

\section{評価}

\subsection{評価環境}

評価にはfern03およびfern04を使用する。
各ノードはIntel Xeon Gold 6530（32コア）を搭載し、
NICとしてMellanox Connect-X7をNUMA 0に接続している。
NUMAノードは1個の構成であり、コア0--1はIRQ処理用に予約し、ベンチマークでは使用しない。
ノード間はInfiniBandで接続される。

\subsection{ノード内通信性能}

Client-Daemon通信（IPC）およびデーモン内デリゲーション（inproc）の性能を評価する。

IPC評価では、QD=1でクライアント数を1--32まで変化させ、
SPSC集約方式とMPSC方式のスループット・レイテンシを比較する。
合わせて、perf c2cによるHITMイベント数を計測し、
キャッシュコヒーレンシオーバーヘッドの定量分析を行う。

inproc評価では、QD=8--32でデーモンワーカースレッド間の全対全通信性能を測定し、
学術論文中のSOTA実装との比較を行う。
% TODO: 比較対象の具体的な論文リストを確定

\subsection{RPC性能}

ノード間RPC通信について、以下の3手法を比較する。
\begin{itemize}
    \item copyrpc: 本提案手法
    \item UCX Active Message: UCXフレームワークによるRPC
    \item Naive send/recv: ibverbs上の素朴なsend/recv実装
\end{itemize}

メッセージサイズ（64B, 256B, 1KB, 4KB）ごとのレイテンシとスループットを測定する。
また、QP数を10から3000まで変化させたスケーラビリティ評価と、
バッチサイズによるdoorbell batching効果の分析を行う。

% TODO: 間に合えば memory polling vs CQ pollingの直接比較

\subsection{統合評価}

benchkvを用いたEnd-to-End評価を行う。
benchkvはrank IDとキーによる単純なキーバリューストアであり、
アーキテクチャの評価を主目的とする。

YCSBライクなワークロードとして、read/write比率を
90\%/10\%、50\%/50\%、10\%/90\%に設定した3パターンで評価する。
メトリクスとして、IOPS、レイテンシ（p50, p99）を測定し、
メモリアクセス指標（HITMイベント数、キャッシュミス率）とIOPSの相関を分析する。

マルチノード構成でのスケーラビリティ評価も行い、
デーモン中継アーキテクチャが大規模環境で有効であることを検証する。

\section{おわりに}

本稿では、Fat node環境におけるスケーラブルなRPC基盤LocustaRPCを提案した。
LocustaRPCは、ノード内IPC、デーモン内デリゲーション、ノード間RDMA通信の全てを
メモリアクセス最適化問題として統一的に捉え、
デーモン中継型アーキテクチャにより接続数削減とQueue depth活用を両立する。

ibverbsのバイパスによるCQポーリング効率化、CLDEMOTEを活用したキャッシュ制御、
sfence集約によるStore buffer最適化など、各通信層においてメモリアクセスパターンを
意識した設計を行った。

% TODO: 評価結果のサマリを記述

今後の課題として、マルチNUMAノード環境での評価、
ファイルシステムやデータ分析など多様なワークロードへの適用、
次世代NIC（RDMAX等）のハードウェア支援との統合が挙げられる。

\begin{acknowledgment}
謝辞をここに記述する．
\end{acknowledgment}

\bibliographystyle{ipsjsort}
\bibliography{filesystem}

\end{document}

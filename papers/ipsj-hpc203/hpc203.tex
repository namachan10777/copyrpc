%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,eabstractは任意)
%% [noauthor]
%%

%\documentclass[submit,techrep]{ipsj}
\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage[dvips]{graphicx}
\usepackage{latexsym}
\usepackage{url}

% ipsjクラスの\doiがDOI中のアンダースコアを処理できない問題を回避
\makeatletter
\renewcommand{\doi}[1]{\url{https://doi.org/#1}}
\makeatother

\begin{document}

\title{LocustaRPC: 次世代リーダーシップマシンのための\\スケーラブルなRPC基盤}

\etitle{LocustaRPC: A Scalable RPC Framework\\for Next-Generation Leadership Machines}

\affiliate{UT}{筑波大学}
\affiliate{CCS}{筑波大学計算科学研究センター}
\affiliate{FUJITSU}{富士通株式会社}

\author{中野 将生}{Masaki Nakano}{UT}
\author{前田 宗則}{Munenori Maeda}{CCS,FUJITSU}[maeda.munenori@fujitsu.com]
\author{建部 修見}{Osamu Tatebe}{CCS}

\begin{abstract}
HPCマシンにおける性能向上は，アクセラレータ
\end{abstract}

\maketitle

\section{はじめに}

ムーアの法則はパイプラインの深化やOut-of-Order実行に代表されるマイクロアーキテクチャの改良，半導体製造プロセスの改良の両輪によって進んできていた．
しかし，物理的限界によってシングルコアでの性能向上が困難になったことで，マルチコア化やアクセラレータによる一部処理の特殊ハードウェア化が進み，
HPCにおいてもアクセラレータ，メニーコアといった構成が採用されてきた．近年では，El capitan，Frontierといったリーダーシップマシンでは % El capitan, Frontierについてそれぞれ設計を発表するProceedingsなどを引用出来るとベスト
アクセラレータとCPUソケットを複数搭載したFat-nodeと呼ばれる構成が採用されている．
日本においても，筑波大学のSiriusスーパーコンピュータ，富岳NEXTにおいてFat-node構成が採用されており， % Siriusについては多分まだ発表はない．富岳NEXTはスライドを引用する
ノード数自体は減少しつつも全体として高い並列性を持ち高度な計算能力が実現される．

一方で，こうした構成ではインターコネクトの物理設計としてはスケーラビリティを確保するものの，
ソフトウェア的にはプロセス数自体はむしろ増加するため通信ライブラリでのスケーラビリティについての課題が残る．
富岳NEXTでは100万オーダーでのプロセス数が想定されており，% 松岡先生のスライドから引用する
ad-hocファイルシステムではPOSIXインターフェースを介した柔軟なI/O要求を処理する必要があるため，どのクライアントがどのサーバにアクセスするかを事前に強く制約しにくい．
このため全体での接続性を維持する設計を採ると，1ノードあたり100万オーダーの接続数受け入れが必要になってくる．
数値計算においては，NCCLやMPIといった通信ミドルウェアでの最適化研究の結果この問題は緩和されているが， % 論文を引用する
ストレージ等のHPCアプリを補助するミドルウェアでの通信では，アクセス先の偏りや通信相手を固定的に仮定しにくく，これらの最適化を直接適用することは難しい．
MPI-IOのようなアクセスパターンの仮定が可能なミドルウェアでは高いスケーラビリティを達成する手法が提案されている一方， % Kohei Hiraga, Kohei Sugiharaらの研究を引用
HPCアプリではPOSIXインターフェースを備える並列ファイルシステムを仮定したアプリも多い．

天文データ解析，機械学習，大気シミュレーションといった高いストレージ性能を要求するアプリへの回答として，
富岳ではLLIOが導入され並列ファイルシステムの性能を補完している．% LLIOを引用
このような計算ノードでの一時的なストレージミドルウェアはad-hocファイルシステムとして研究が行われている．% Gecko, CHFS, BeeOND, DeltaFSあたりは最低でも引用する．
ad-hocファイルシステム研究では主に並列ファイルシステムでボトルネックとなりがちであった
メタデータのスケーラビリティ改善が達成されてきた．
ただ，これらの研究は主に分散アルゴリズムが主眼であり，通信層の最適化は副次的なものとなっている．

こうした通信層についてのスケーラビリティ上の課題は，データセンター内データベースの最適化問題として先行して研究が行われてきた．
一方でHPC向けストレージミドルウェアでは，扱うデータ量や同時通信の規模が大きく，KVSに比べてクエリの単純化も難しいため，通信アーキテクチャ自体の改良がより重要になる．
HPCで広く用いられるInfinibandネットワークでは，
高速かつ高信頼性のRC接続による通信によって分散計算やストレージミドルウェアが実装されるが，
このRC接続の状態管理がネットワークインターフェースにおいてスケーラビリティの課題を抱えていることが報告されている． % ScaleRPCを引用する．あとKalia先生も多分報告していたはず．ScaleRPC，RDMAX，UD系RPCはこうした文脈で研究されているので．
また，単純なネットワーク性能のみならず通信のためのメモリリソースも計算用メモリの圧迫としてアプリ性能に影響を与えうる． % ここ引用がほしい
従来のHPC向けRPCフレームワークでは，クライアントが直接接続を持つためノード内プロセスの増加がそのまま
リソースの増加に直結する構造であり，Fat-nodeになりノード数が減少しても問題は解決されない．

本研究では，大規模なHPCアプリ実行時に，ad-hocファイルシステムのようなミドルウェアで接続状態の増大により通信性能低下とメモリ消費増大が同時に生じる点を課題とする．
とりわけメモリ消費増大は，HPCノードでOOM killerによる強制終了を誘発し，計算用メモリを直接圧迫するため深刻である．
この課題に対し，POSIX共有メモリを用いたノード内通信によってコネクションを集約し，従来のHPC向けRPCより少ないリソースでノード間通信を実現するアーキテクチャを提案する．
さらに，提案アーキテクチャによって検証用Key-Valueストアを実装して，設計の有効性を検証する．
本稿の貢献は以下の通りである．

\begin{enumerate}
	\item 共有メモリを用いた通信集約アーキテクチャ
	\item 共有メモリ上に集約されたリクエスト転送を目的としたRPCの設計とフローコントロール
	\item リクエスト代行におけるバッチング崩壊問題と，その対処のための制御アルゴリズム
	\item ベンチマーク用Key-Value Storeによる性能比較
\end{enumerate}

\section{背景}

\subsection{並列ファイルシステムの発展とad-hocファイルシステム}

\subsection{RDMA通信}

\section{関連研究}

\subsection{eRPC, ScaleRPC, ...} % あともう一個くらいソフトウェアによるRPCスケーラビリティ研究を出す

% これらの研究はスケーラビリティ向上は達成している
% 一方で，これらの研究はクライアントがフルの通信用Infiniband/RoCEリソースを持つ
% クライアントは別に高機能な通信を行うわけではないので，メモリオーバーヘッドがある．
% QPとかのメモリ使用量を概算して記述する　
% ScaleRPCではスケーラビリティが問題として，RCを減らそうとしているが，ad-hoc filesystemでは全てのノードにDaemonも展開されている
% そのため，サーバ側の問題を解消しても同居するクライアントが性能問題を引き起こす(ここは要考察)．
% しかしScaleRPCは計算ノードへのDaemon展開は想定していないから，ScaleRPCが悪いというかad-hoc filesystemと噛み合わないだけ
% eRPCはUDなのでかなりマシ．NICリソースによる性能低下は起きない
% でも，ad-hoc filesystemとかでは，大メッセージの通信が必要なのでやっぱりRCがほしい
%

\subsection{mRPC}

\begin{acknowledgment}
本研究の一部は，JSPS科研費JP22H00509，
NEDO（国立研究開発法人新エネルギー・産業技術総合開発機構）の委託事業「ポスト5G情報通信システム基盤強化研究開発事業」（JPNP20017），
JST CREST JPMJCR24R4，
筑波大学計算科学研究センター学際共同利用プログラム，および富士通との特別共同研究の結果得られたものです．
\end{acknowledgment}

\bibliographystyle{ipsjsort}
\bibliography{filesystem}

\end{document}

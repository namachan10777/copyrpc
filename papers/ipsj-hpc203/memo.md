# 目標

- ノード内リクエスト集約により、ノード間通信においてQueue depthを生かしつつConnectionを減らして
負荷減少しつつレイテンシ隠蔽で高速化
- ad-hoc filesystem, kvsなどを前提として、HPCアプリにおいて補助的に展開されるミドルウェアを想定
- 大規模ノードでのscalabilityを確保する

ストーリーとしては、Fat nodeにおける通信爆発を解決するためにノード内通信をやる。
丁寧なipc/inprocの実装選択と、CPU間転送の考察によって最適化する。
その上で、IPCとHITMといったキャッシュ関連のperfパラメータを手掛かりにアクセスパターンを最適化する。
RDMA NICは十分に高速なので、NICの基本（DCでAV切り替えると性能が遅くなる。これはDCは高速接続するRCに近いから。UDでは起きないが、代わりにone-sided通信が出来ない）などを踏まえ、NIC friendlyな設計を意識すれば、少数コアでのNIC操作における主要なボトルネックは結局CPUがどれだけ効率的にメモリ操作出来るかにかかってくる。なので、全てをメモリ操作の最適化として捉えて高速化を図る。接続数はad-hoc filesystemみたいなワークロードに限れば、アーキテクチャレベルでの改善の方が簡単だしNICの性能を活かしやすい。

## アーキテクチャ全体像

- 1ノードに1デーモンプロセス（複数スレッドあり）
- clientは全てのノード内Daemonと接続し、適切な中継Daemonへリクエストを配送する
- Connectionはrank id mod デーモンスレッド数でシャーディングされている
- client → (ipc) → daemon → (inproc: daemon内delegation) → daemon thread → (copyrpc: ノード間) → remote daemon

## 構成要素

- ipc
  - client/daemon通信を行う
  - QD=1、client多数（32程度、ノード内の全計算コア）状況への最適化
  - client/daemon共有メモリとそのアドレス変換をサポートする
  - 性能評価: QD=1、多数clientのベンチマークによって性能評価。多数のSOTA実装との比較（学術論文中心、後で補足）
- inproc
  - daemon内delegation
  - 性能評価: 中QD(8とか32くらい）、daemonワーカースレッド同士の全対全ベンチマークによって性能評価。多数のSOTA実装との比較（学術論文中心、後で補足）
- copyrpc
  - daemon/daemonのノード間通信を行う
  - RDMA Write with Immediateでのバッファ転送が主体。送信リングに書いてから送るため1回コピーが入る
  - end-to-endのクレジットベース フロー制御でハングを起こさない（copyrpcクレート内で実装）
  - バッファリングと一括転送により、SQ/RQのWQE消費を減らして、WQE枯渇によるRNR retryを回避
  - 受信完了検知: 受信バッファ上のフラグをポーリングする設計（memory polling）が多いが、本実装ではCQポーリングによるイベント集約で多数QPでの効率化を狙う。memory pollingが速いという主張にはibverbsラッパのオーバーヘッドが影響している可能性が大きく、直接mlx5デバイスを叩けばCQポーリングは十分高速
  - UCXとの比較、naive send/recvとの比較。（間に合えば memory pollingとの比較）
- benchkv
  - rank id + keyによるシンプルなKVS。KVS自体は軽量で、アーキテクチャの評価が目的
  - rankはMPIランクに相当するrank id。daemonスレッド番号とは直接対応しない

## 評価項目

- mpsc性能評価
  - spsc集約 v.s. mpsc
  - 低QD or 高QD
  - HITM（`perf c2c`で計測）: どのくらいキャッシュコヒーレンシプロトコルが走ってどのくらい他の操作に影響する？
  - Store bufferの節約: spsc実装の違いにより書き込み量が変わるため分析可能
- RPC性能評価
  - raw send recvとUCXとcopyrpcで勝負
- 統合評価
  - IPCとかのキャッシュ関連パラメータとIOPSの評価
  - YCSB like benchmark: read/write比率を変えて比較（50%/50%, 10%/90%, 90%/10%など）
  - ここの評価を逆算して最適化

## その他小噺（実装部分とかに記載かな）

- mlx5デバイス使うならibverbsやめた方がいいよ。直接メモリさわればメモリアクセスを最適化出来るし、オーバーヘッドも減る。ibverbsの具体的問題: 余計なコピーが多い、spin lockが特に大きいボトルネック
- 実はキャッシュコヒーレンシプロトコルでのデータ転送よりもCLDEMOTEでL3に落としてそれを他のコアで読む方がレイテンシ低かったりするよ（NUMA跨ぎの評価は未実施。現在の実験環境はNUMAノード1個）
- QP数による性能低下はConnect-X7とかだとまた話が変わってきているよ。別に1000個くらいなら割と動くよ

## 評価環境

- マシン: fern03/fern04, Intel Xeon Gold 6530, 32コア
- NIC: Connect-X7（NUMA 0に接続）
- NUMAノード: 1個の環境で実験
- コア0,1はIRQ処理用、ベンチマークでは避ける

## TODO

- [ ] はじめにの構成を再設計する
  - 背景で出すべき用語（特にad-hocファイルシステム）をはじめにで先出ししない
  - はじめにのスケーラビリティ課題はScaleRPCやKaliaらの先行研究を引用して一般論として述べる
  - ad-hocファイルシステムは背景で具体例として導入する
  - MPIで相対的に問題化しにくい点も背景側で整理する
- [ ] はじめにでは「ミドルウェアで問題化する」程度に留める
  - LLIOはミドルウェア例として触れる
- [ ] はじめにの分量を削減し，背景へ説明を移す
- [ ] 文脈を「計算を補助するミドルウェアが計算ノードメモリを過剰消費すべきでない」に寄せる
- [ ] はじめに末尾は貢献列挙ではなく，論文構成（本稿の流れ）を記述する
- [ ] 本文中コメントに従って未解決の引用を埋める

- [ ] inproc/ipcのSOTA比較対象の具体的な論文リストを補足
- [ ] YCSBベンチマークの詳細設計を補足
- [ ] copyrpcのクレジットベースフロー制御の実装詳細を実コードから確認・記載
- [ ] CLDEMOTEのNUMA跨ぎでの評価が必要か検討
